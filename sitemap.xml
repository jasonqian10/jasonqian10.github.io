<search>
    
     <entry>
        <title>Intellij idea搭建solr源码debug调试环境</title>
        <url>https://jasonqian10.github.io/post/solr-debug-idea-build/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>debug</tag><tag>Intellij idea</tag>
        </tags>
        <content type="html"> solr源码debug环境对于研究solr和Lucene源码很有必要，本文总结本人搭建的详细步骤。
clone代码 git clone https://github.com/apache/lucene-solr.git
下载后阅读根目录下README.md。
下载ant依赖 如果本地没有安装过ant的请自行安装。
在项目根目录下执行命令
ant ivy-bootstrap
自动下载ant依赖包。第一次执行需要较长时间，显示BUILD SUCCESSFUL则成功。
 这一步只需要执行一次，后面不需重复执行。
 ant项目转换成maven项目 项目根目录下执行命令
ant idea
 注意：执行完此步骤才可以用intellj idea打开项目。未转换先打开了会有问题。
 编译项目 进入项目根目录 solr/ 目录，执行命令
ant server
idea运行solr server 在idea上运行solr server是实际启动的是embedSolrServer。
 先准备一个solr_home，放在solr/server/ 目录下
 在项目 solr/solrj/src/test/org/apache/solr/client/solrj/StartSolrJetty.java 中增加上面准备的solr_hom的路径；设置webapp war路径
   run StartSolrJetty.java，然后访问 localhost:8983/solr   对源码打断点，debug StartSolrJetty.java 即可进行源码调试  导入自定义jar 如果有基于源码自定义的插件jar，也可以把插件代码导入到源码项目中进行debug。
</content>
    </entry>
    
     <entry>
        <title>搜索引擎中相似度算法TF-IDF和BM25</title>
        <url>https://jasonqian10.github.io/post/nlp-tf-idf-bm25/</url>
        <categories>
          <category>Lucene</category>
        </categories>
        <tags>
          <tag>TF-IDF</tag><tag>BM25</tag>
        </tags>
        <content type="html">  前言 当我们使用搜索引擎时，它总是会把相关性高的内容显示在前面，相关性低的内容显示在后面。那么，搜索引擎是如何计算关键字和内容的相关性呢？这里介绍2种重要的相似度算法：TF-IDF和BM25。
TF-IDF是Lucene上一代（6.0以前）相似度算法，BM25是Lucene新一代（6.0以后）正使用的相似度算法。
先举个例子。假如，我们想找和“Lucene”相关的文章。可以想一下，那些内容里只出现过一次“Lucene”的文章，有可能是在讲某种技术，顺便提到了Lucene这个工具。而那些出现了两三次“Lucene”的文章，很可能是专门讨论Lucene的。通过直觉，我们可以得出判断：关键字出现的次数越多，文档与关键字的匹配度越高。
TF Term Frequency，缩写为TF。通常叫做“词频”，表示文档中关键字出现的次数。
 通常TF越大，相关性越高。
 但是，你可能会发现一个问题。例如一篇小短文里出现了一次“Lucene”，而一部好几百页的书里提到两次“Lucene”，此时我们就不能说后者相关度更高了。为了消除文档本身大小的影响，在计算TF时引入文档长度这个参数，做文档长度标准化
TF socre ＝ 某个词在文档中出现的次数 ／ 文档的长度 举例：某文档D，长度为200，其中“Lucene”出现了2次，“的”出现了20次，“原理”出现了3次，那么
TF(Lucene|D) = 2/200 = 0.01 TF(的|D) = 20/200 = 0.1 TF(原理|D) = 3/200 = 0.015 其中我们发现“的”这个词的比重非常高，搜索引擎中一般把这种“的、地、得”这些虚词去掉，不计算其分数。
IDF Inverse Dcument Frequency, 缩写为IDF。通常叫做“逆文档频率”，其定义为
IDF = log(语料库的文档总数 / 包含该词的文档数 &#43; 1) 可见包含该词的文档数越小，分母就越小，IDF越大；该词越常见，分母就越大，IDF越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。
举例：世界上文档总数位100亿，&amp;rdquo;Lucene&amp;rdquo;在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
IDF(Lucene) = log(100亿/1万&#43;1) = 19.93 IDF(原理) ＝ log(100亿/2亿&#43;1) ＝ 5.64 “Lucene”重要性相当于“原理”的3.5倍，可见“Lucene”更能表征这篇文档。
TF-IDF TF-IDF算法的相似度公式就是TF和IDF的加权求和
simlarity = TF1*IDF1 &#43; TF2*IDF2 &#43; ... &#43; TFn*IDFn 上面的例子最终相似度得分为
simlarity(Lucene的原理|D) = 0.01*19.93 &#43; 0 &#43; 5.64*0.015 ＝ 0.2839 其中，“Lucene”占了总分70%的权重，“原理”仅占总分30%的权重。
Lucene中的TF-IDF Lucene对TF-IDF算法做了适当调整，它的相似度公式为
simlarity = log(numDocs / (docFreq &#43; 1)) * sqrt(tf) * (1/sqrt(length)) 各参数含义
 numDocs： 索引的文档总数量 docFreq： 包含关键字的文档数量 tf：关键字在一篇文档中出现的次数。 length：文档的长度  上面的公式在Lucene打分计算时会被拆分成三个部分：
IDF Score = log(numDocs / (docFreq &#43; 1)) TF Score = sqrt(tf) fieldNorms = 1/sqrt(length)  log()，sqrt()主要是为了降低分值大小，缩写文档之间分数差距
fieldNorms是对文本长度的归一化(Normalization)，为了消除文档长度对分数的影响
 所以，上面公式也可以表示成
simlarity = IDF score * TF score * fieldNorms Lucene在score打分时会遍历匹配到的每篇文档计算每篇文档的score，每篇文档分别计算这三部分分值。最后默认根据score sort。
BM25,下一代的TF-IDF 新版的Lucene不再把TF-IDF作为默认的相关性算法，而是采用了BM25(BM是Best Matching的意思)。BM25是基于TF-IDF并做了改进的算法。先来看下改进之后的BM25算法
simlarity = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) * ((k1 &#43; 1) * tf) / (K &#43; tf) 上面的公式在Lucene打分计算时会被拆分成两部分：
IDF Score = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) TF Score = ((k1 &#43; 1) * tf) / (K &#43; tf)  这里k1和K下面展开讨论
  #### TF项比较  两者公式如下
传统 TF Score = sqrt(tf) BM25的 TF Score = ((k1 &#43; 1) * tf) / (K &#43; tf) 传统的TF值理论上是可以无限大的。而BM25与之不同，它在TF计算方法中增加了一个常量k1和K，用来限制TF值的增长极限，BM25的 TF Score的范围会控制在[0, k1&#43;1]。其中K
K = k1(1 - b &#43; b*dl/avgdl) 取值k1，b是调节因子，Lucene中默认是k1=2，b=0.75，用户可以自定义。dl为文档的长度，avgdl为所有文档的平均长度。
两者的分布曲线来看，随着tf增大，BM25的TF会接近k1&#43;1，传统的TF会无限变大。
TF中还引入了文档的长度dl，所有文档的平均长度avgdl，这里假设L = dl/avgdl，下面是不同L的条件下，词频对TFScore影响的走势图
从图上可以看到，文档越短，它逼近上限的速度越快，反之则越慢。这是可以理解的，对于只有几个词的内容，比如文章“标题”，只需要匹配很少的几个词，就可以确定相关性。而对于大篇幅的内容，比如一本书的内容，需要匹配很多词才能知道它的重点是讲什么。所以在专利检索中相关性通常的顺序 TTL &amp;gt; ABST &amp;gt; CLMS &amp;gt; DESC。
关于参数b的作用：公式中L前面有个常系数b，如果把b设置为0，则L完全失去对评分的影响力。b的值越大，L对总评分的影响力越大。
 IDF项比较  两者公式如下
传统的 IDF Score = log(numDocs / (docFreq &#43; 1)) BM25的 IDF Score = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) 从分布曲线来看两者走势基本一致。
TF-IDF vs BM25 传统的TF-IDF是自然语言搜索的一个基础理论，它符合信息论中的熵的计算原理，你观察IDF公式会发现，它与熵的公式是类似的。实际上IDF就是一个特定条件下关键词概率分布的交叉熵。
BM25在传统TF-IDF的基础上增加了几个可调节的参数，使得它在应用上更佳灵活和强大，具有较高的实用性。
</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】hadoop调优经验</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-3/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag>
        </tags>
        <content type="html"> 【Hadoop索引】本篇系列第三篇。主要介绍Hadoop调试过程中的一些问题以及解决方法的总结。
主要有以下几类问题
 资源分配问题
 性能调优问题
 Yarn框架运行机制问题
 Solr索引问题
  资源分配问题 MapReduce集群任务分配机制 举例：一套机型为c4.4x的16个node节点的emr集群为例
 c4.4x机器配置16核，30G内存
 MapReduce集群最多可运行多少个container资源？ 默认
以机器的vcode数量作为container数量，总 16* 16=256 container，也就是集群做多可以同时跑256个task。每个container的含有资源为 1core，30*0.75&amp;frasl;16=1.4G。
 emr默认会用机器75%的内存来分配
 实际并发container数量 阶段 | 实际设定的内存 | 单node并发的container数量 | 集群总并发container数量 -|-|-|- Map | 3G | 30*0.75&amp;frasl;3=7 |7*16=112 Reduce | 16G | 30*0.75&amp;frasl;16=1 |1*16=16
MapReduce job会被拆分成多少个map/reduce task？ 阶段 | task数量 -|-|-|- Map | input size/ hdfs block size(split size) Reduce | mapreduce.job.reduces设定 &amp;gt; 默认情况下hdfs block size(split size)为64M，可以根据业务需要调整大小来控制Map task数量。
内存溢出问题 问题现象：提示“Container is running beyond physical memory”、“running beyond virtual memory limits”
原因：container设定的物理内存、虚拟内存不足导致失败。
解决：
1）一般内存溢出大部分是用户自己代码问题，可以优化代码是内存使用降低
2）如果必须需要使用大量内存，可以通过配置提高内存 参数 |说明 -|- mapreduce.map.memory.mb |Map container内存 mapreduce.map.java.opts |Map jvm内存，一般为Map container内存的80% mapreduce.reduce.memory.mb |Reduce container内存 mapreduce.reduce.java.opts |Reduce jvm内存，一般为Reduce container内存的80% yarn.nodemanager.vmem-pmem-ratio |虚拟内存和物理内存比值，默认5。如果虚拟内存需要太大，可以通过提高这个参数 yarn.nodemanager.vmem-check-enabled |虚拟内存检查，默认true。false可以关闭虚拟内存阈值check，一般不建议修改。 yarn.nodemanager.pmem-check-enabled |物理内存检查，默认true。false可以关闭物理内存阈值check，一般不建议修改。
性能调优问题 shuffle阶段性能调优配置 参数 |说明 -|- mapreduce.job.reduce.slowstart.completedmaps | 这个参数表示map完成多少进度后开始启动reduce container。默认是0.5。建议设置0.95，因为reduce container过早启动会抢占map的资源，导致并发map的数量减少，reduce正式开始跑需要等待map 100%完成，启动早无太大实质效果，只是决定早点开始拷贝map output。 mapreduce.reduce.shuffle.merge.percent | reduce 复制的数据先写到 reduce 任务的 JVM 内存缓存区，这个参数表示内存缓存区占reduce jvm比重阀值，超过该比例将进行合并和溢写磁盘。默认是0.66，建议设小以免shuffle时reduce jvm内存爆。 mapreduce.reduce.shuffle.memory.limit.percent | 单个shuffle线程能使用的内存限额，默认是0.25，并发线程数多于4之后容易内存爆掉，建议设置小于0.15。 mapreduce.reduce.shuffle.parallelcopies | reduce并行copy maptask输出文件的线程数量，默认是5，可以配置（根据集群节点间的带宽） mapreduce.reduce.shuffle.read.timeout | shuffle copy的超时时间，默认是180000。如果集群节点间带宽较低，可以增加超时时间，否则可能会copy timeout。 |
|
Yarn框架运行机制问题 Task运行超时被kill掉 问题：Task由于代码堵塞或者业务逻辑执行时间过长，长时间不结束，TaskTracker把这个Task kill掉。
原因：每个Task会定期向TaskTracker汇报进度，如果进度不变则不汇报，这样一旦达到超时限制，TaskTracker会杀掉该任务，并将任务状态KILLED汇报给YARN,从而重新调度该任务。
解决：
1）Task主动上报状态，使得TaskTracker知道Task正在运行。可以调用MapReduce的统计相关的sdk，如TaskAttemptContext.progress()、TaskAttemptContext.getCounter()等sdk。
2）mapreduce.task.timeout，mr程序的task执行情况汇报过期时间，默认600000(10分钟)，可以设大。
shuffle copy time out 问题：shuffle阶段reducer copy各个节点上map的输出文件，报“Failed to connect to xxx with 1 map outputs java.net.SocketTimeoutException: Read timed out”
原因：由于机器ebs带宽瓶颈，copy超时。
解决：这个原因其实是机器性能问题，无法直接进行提升。可以通过以下方式进行规避。
1）mapreduce.reduce.shuffle.parallelcopies
参数是copy并发线程数量，建议设小
2）mapreduce.reduce.shuffle.read.timeout
参数表示shuffle copy的超时时间，默认是180000，建议设大
3）mapreduce.reduce.maxattempts，mapreduce.map.maxattempts
参数表示reduce、map task失败重试次数，这种网络拷贝问题是并发时偶现问题，增加重试次数
Solr索引问题 reduce阶段做solr index和optimize爆内存 问题：solr index和optimize时reduce container内存爆掉
原因：solr index默认使用MMapDirectoryFactory，这个目录实现类适用虚拟内存和内核中一个叫 mmap 的特性来访问存在磁盘上的文件。它允许 Lucene 直接访问 I/O 缓存，当不需要准实时搜索时，这个目录实现是一个非常不错的选择。MMap由于使用了memory mapping，会使用到跟索引总大小近似的virtual memory。
解决：替换成NIOFSDirectoryFactory，在solr index和optimize过程中进行内存使用基本平稳。
</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】Mapreduce进行solr索引实战</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-2/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>solr</tag>
        </tags>
        <content type="html"> 【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第二篇，介绍如何应用Hadoop到solr index。
为什么使用Hadoop来做solr索引？ 这与Hadoop特点有关，hadoop适用于处理大数据并行计算，提高solr索引速度。Hadoop的MapReduce计算框架分map阶段和reduce阶段。map阶段转换solr document，reduce阶段进行solr index操作。MapReduce流程契合solr索引的流程。
如何用MapReduce做solr索引？ 网上现在使用MapReduce做solr索引的方式实战比较少，常见的尝试有如下两种
 Cloudera MapReduceIndexerTool  cloudera提供了基于morphline的索引工具SDK，solr也做了集成，在solr/contrib/map-reduce包。入口类为 MapReduceIndexerTool，详细使用可以参考
https://docs.cloudera.com/documentation/enterprise/latest/topics/search_mapreduceindexertool.html
 running solr on HDFS  这个是solr支持的索引到HDFS上，其实与MapReduce没有啥关系，只是存储文件系统换成了HDFS，细节见
https://lucene.apache.org/solr/guide/6_6/running-solr-on-hdfs.html
作者起先尝试过Cloudera MapReduceIndexerTool的方式，发现有些定制化的功能无法满足，而且solr在6.6版本之后移除了map-reduce模块（https://issues.apache.org/jira/browse/SOLR-9221 ），原因是solr与Hadoop版本兼容问题，所以最后放弃使用了。但是solr/contrib/map-reduce中很多源码是值得参考的，我们在开发过程了也借鉴了很多。
运行流程  备注: 我们使用aws emr做Hadoop集群，所以会使用一些 aws的服务
 流程其实很清晰，主要有以下几部分
 Input  做索引的输入文件，存在在aws s3或者hdfs。
job client提交job任务，初始化MapReduce job object。
 Map  map阶段完成solrDoucment的转换，其map的输出为，其中shardNum表示solrDocument所在solr集群中哪个shard。因为我们使用embedSolr作为solrServer进行index，并未使用https的方式进行index，所以分shard的逻辑需要在map中进行计算。算出shardNum，作为map输出的key。这样在后续shuffle，sort过程中MapReduce框架会把key相同的所有solrDoucment拷贝到同一reduce task的节点上。
 Reduce  包含前置流程shuffle和sort，目的是把Hadoop集群各个节点上map输出的solrDoucment按key的不同拷贝并排序到对应的reduce task节点上。
Reduce task就纯粹完成solr index，reduce container的jvm中启动EmbeddedSolr作为solr server。
 output  solr索引文件支持输出到hdfs或者disk（只需在solrconfig.xml中配置即可），两种方式作者均尝试过，index速度方面，存储到disk更快一些。如果对index速度要求不是很高，建议使用hdfs，因为使用了Hadoop，最好使用它的分布式存储系统。
代码实现 job client Job client main()方法
//初始化Job object Job.getInstance(conf, &amp;#34;solr index&amp;#34;) Configuration conf = job.getConfiguration(); //设置MapReduce入口类（job client类） job.setJarByClass(Runner.class); //设置MapReduce输入文件 FileInputFormat.addInputPath(job, new Path(INPUT_PATH))); //设置blocksize大小，INPUT_PATH会按照splitSize大小进行分割成多个 FileInputFormat.setMinInputSplitSize(job, splitSize); FileInputFormat.setMaxInputSplitSize(job, splitSize); //SolrOutputFormat.java继承FileOutputFormat.java。solr/contrib/map-reduce包中存在这个类 job.setOutputFormatClass(SolrOutputFormat.class); //map output的key类型 job.setOutputKeyClass(IntWritable.class); //设置map多线程 MultithreadedMapper.setMapperClass(job, SolrMapper.class); MultithreadedMapper.setNumberOfThreads(job, MAP_THREAD_NUM); job.setMapperClass(MultithreadedMapper.class); //map output的value类型 job.setOutputValueClass(SolrInputDocumentWritable.class); //优先使用jar包classpath的类，其次才会使用Hadoop环境中lib的classpath类。为了解决不同版本类冲突 conf.setBoolean(&amp;#34;mapreduce.job.user.classpath.first&amp;#34;, true); //设置reduce task数量 job.setNumReduceTasks(REDUCE_TASK_NUM)); 如上源码是初始化job client核心代码。如果jar包运行过程中涉及一些配置文件，需要传至各个node节点，则可以参考 SolrOutputFormat#setupResourceCache这个方法。
map SolrMapper.java
public class SolrMapper extends Mapper&amp;lt;LongWritable, Text, IntWritable, SolrInputDocumentWritable&amp;gt; { @Override protected void setup(Context context) throws IOException { //初始化  } @Override protected void cleanup(Context context) { } @Override public void map(LongWritable key, Text value, Context context) throws InterruptedException, IOException { SolrInputDocument doc = xxx; //solr document转换逻辑  int shardOrd = xxx; //solr分shard逻辑，确定doc的分片  //统计  context.getCounter(Counters.MAPPER).increment(1); //map outout  context.write(new IntWritable(shardOrd), new SolrInputDocumentWritable(doc)); } } 其中SolrInputDocumentWritable.java来自solr/contrib/map-reduce包。
reduce SolrReducer.java
public class SolrReducer&amp;lt;K, V&amp;gt; extends RecordWriter&amp;lt;K, V&amp;gt; { private SolrClient solrClient; public SolrReducer(TaskAttemptContext context) { //初始化  this.solrClient = SolrUtil.createEmbeddedSolr(); } /** * Write a record. This method accumulates records in to a batch, and when * items are present flushes it to the indexer. The writes * can take a substantial amount of time, depending on. If * there is heavy disk contention the writes may take more than the 600 second * default timeout. */ @Override public void write(K key, V value) { SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value; SolrInputDocument sid = sidw.getSolrInputDocument(); solrClient.add(sid); } @Override public void close(TaskAttemptContext context) { if (solrClient != null) { solrClient.close(); } }</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】我理解的hadoop</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-1/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>solr</tag>
        </tags>
        <content type="html"> 【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第一篇，介绍我学习使用Hadoop后对其的理解。
Hadoop概念 官方概念 The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.
总结：处理大数据、分布式集群、可编程计算框架
Hadoop 的核心组件 Hadoop由1.0和2.0两个架构版本，2.0最大的变化就是引入YARN这个资源调度框架，更加精细化的分配和调度资源。这里主要以当前最新的2.0展开。感兴趣的同学可以自行了解一下两者差异。
 HDFS：分布式文件系统
 Mapreduce：分布式运算编程框架
 YARN：运算资源调度系统
  简单描述一下三者关系：自定义的MapReduce程序（Java进程），通过YARN框架分配资源来运行这个MapReduce程序，MapReduce程序中输出存储到HDFS。
YARN框架 Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。它将资源管理和处理组件分开，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。
YARN框架几个重要组件/概念  ResourceManager(RM)  ResourceManager包含两个主要的组件：定时调用器(Scheduler)以及应用管理器(ApplicationsManager (AsM))。
 定时调度器(Scheduler)：这个组件完全是插拔式的，用户可以根据自己的需求实现不同的调度器，目前YARN提供了FIFO、容量以及公平调度器。这个组件的唯一功能就是给提交到集群的应用程序分配资源，并且对可用的资源和运行的队列进行限制。Scheduler并不对作业进行监控，并且它不保证会重启由于应用程序本身或硬件出错而执行失败的应用程序。
 为所有AM分配资源
 应用管理器(ApplicationsManager (AsM))：这个组件用于管理整个集群应用程序的application masters，负责接收应用程序的提交；为application master启动提供资源；监控应用程序的运行进度以及在应用程序出现故障时重启它。
 用于管理所有AM
   NodeManager(NM)  是YARN中每个节点上的代理，它管理Hadoop集群中单个计算节点，根据相关的设置来启动容器的。NodeManager会定期向ResourceManager发送心跳信息来更新其健康状态。同时其也会监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。
 ApplicationMaster(AM)  ApplicationMaster是应用程序级别的，每个ApplicationMaster管理运行在YARN上的应用程序。YARN 将 ApplicationMaster看做是第三方组件，ApplicationMaster负责和ResourceManager scheduler协商资源，并且和NodeManager通信来运行相应的task。ResourceManager 为 ApplicationMaster 分配容器，这些容器将会用来运行task。ApplicationMaster 也会追踪应用程序的状态，监控容器的运行进度。当容器运行完成， ApplicationMaster 将会向 ResourceManager 注销这个容器；如果是整个作业运行完成，其也会向 ResourceManager 注销自己，这样这些资源就可以分配给其他的应用程序使用了。
 Container容器  Container是一个逻辑上的概念。与特定节点绑定的，其包含了内存、CPU磁盘等逻辑资源。不过在现在的容器实现中，这些资源只包括了内存和CPU。容器是由 ResourceManager scheduler 服务动态分配的资源构成。容器授予 ApplicationMaster 使用特定主机的特定数量资源的权限。ApplicationMaster 也是在容器中运行的，其在应用程序分配的第一个容器中运行。
YARN框架执行流程 1.Client执行main()函数中runjob()，向yarn提交job，开启作业。
 这里client jvm是个独立运行的Java进程，跑在Hadoop集群的master节点上。
 2.Client向RM发送作业请求同时RM将作业id以及jar包存放路径返回给Client。
3.Client会把Jar路径为前缀作业id为后缀作为唯一存放路径，将jar包以及输入分片写入到HDFS集群中。
 2,3两步都是Hadoop框架来做的，用户只需执行1即可。注意，这里框架只会默认上传MapReduce程序jar包和输入分片文件。如果用户运行MapReduce程序过程中还需其他的jar包、配置文件等，需要在1中自行上传，以及在task运行起来后自行获取。
 4.Client再次将Job对象（包含2，3存放的路径）提交给RM。
5.RM将其放入调度器，向NM发送命令，NM开启MRAPPMaster进程。
6.MR初始化job。
7.获取3中写入到HDFS中的输入分片。
 每个输入分片会创建一个map任务对象
 8.拿到输入分片后向RM的资源调度器请求分配资源来运行map、reduce task。
9.一旦RM的资源调度器为任务分配了container，AM就通过与NM通信来启动container。
10.该任务由主类为YarnChild的Java进程执行。但是它在运行之前会将需要的资源文件本地化。这里就会去HDFS获取程序运行所需的相关文件。
 在3中补充提到与框架无关的文件需要用户自己去上传和下载。文件是存放在HDFS上，依赖Hadoop的分布式缓存机制进行上传和下载。达到的效果是把用户需要使用的文件分发到各个NM节点。
 11.运行map任务或者reduce任务。
MapReduce模型  图片来自 https://blog.csdn.net/aijiudu/article/details/72353510
 MapReduce将复杂的并行计算过程高度的抽象到了两个函数：Map函数和Reduce函数。
   函数 输入 输出 说明     Map &amp;lt;键1,值1&amp;gt;
如&amp;lt;行号,pid&amp;gt; 如 1.将split后的每个input数据集进一步解析成一批对，输入到map函数进行处理
2.每个输入都会输出，这是中间结果，作为map的输出存储到磁盘。等到job完全结束才会删除。   Reduce 如 写hdfs/disk如solrclient.add() 这里的reduce函数无返回对象，直接在函数内部进行最终结果的输出。    MapReduce的核心是“分而治之”策略。数据在其MapReduce的生命周期中过程中需要经过六大阶段，Input、Split、Map、Shuffule、Reduce和Output。这里讲一下最核心的三个。
Map阶段 1.输入文件进行split成小文件，被每个map任务使用。
2.由程序内的InputFormat(默认实现类TextInputFormat)来读取外部数据，它会调用RecordReader(它的成员变量)的read()方法来读取，返回k,v键值对。k,v键值对传送给map()方法，作为其入参来执行用户定义的map逻辑。
 MapReduce框架默认实现
 3.map()方法输出结果也是k,v键值对。对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。用户可以自定义partition逻辑。
4.Map的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。环形缓冲区其实就是一个数组，后端不断接受数据的同时，前端数据不断被溢出，长度用完后读取的新数据再从前端开始覆盖。这个缓冲区默认大小100M，可以通过MR.SORT.MB(应该是它)配置。
5.Map按照定义的partitioner分区，每个分区的输出value进行sort排序。
 若有combiner会先执行combiner。combiner作用是预合并，减少输出文件数量和大小。sort也是为了后面merge时效率更高。
 6.按照定义的partitioner分区spill(溢出)到文件（对于某一个小文件，其key都是来自一个partitioner分区），这些文件仍在map task所处机器上。
7.小文件执行merge(合并)，形成partitioner分区内有序的大文件。这些大文件仍在map task所处机器上。
 一个map最终会输出一个output文件，里面存储了所以value值的有序集合（会有不同key的value）。这里输出的文件可以选择使用压缩。
 Reduce阶段 1.copy数据。Reduce从AM那边获取本Reduce的数据存放在哪些Map里，到对应的Map机器去拉取文件。数据被reduce提走之后，map机器不会立刻删除数据，这是为了预防reduce任务失败需要重做。因此map输出数据是在整个作业完成之后才被删除掉的。
2.Merge、Sort、spill。这个过程与Map阶段类似，reduce将copy过来的数据文件加载到内存，进行merge和sort，当内存缓冲区的使用达到阈值后开始往磁盘spill。这个过程会循环多次，最终得到一份有序的reduce输入看k,v键值对文件。
3.通过GroupingComparator()分辨同一组的数据，把他们发送给reduce(k,iterator)方法。
4.调用context.write()方法，会让OutPurFormat调用RecodeWriter的write()方法将处理结果写入到数据仓库中。
 如何输出，输出到哪，都可以在RecodeWriter里自定义。
 Shuffle 这里讲一下shuffle。这个阶段是MapReduce框架中间过程数据混洗阶段，交叉在map和reduce的过程中，包括数据分区，排序，局部聚合，缓存，拉取，再合并排序等阶段。从Map阶段的3~7以及Reduce阶段的1~2都是shuffle阶段。
HDFS Hadoop Distributed File System(hadoop分布式文件系统)，架构图如下。
有四部分组成
Client 职责：
 文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。
 与 NameNode 交互，获取文件的位置信息。
 与 DataNode 交互，读取或者写入数据。
 Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS。
 Client 可以通过一些命令来访问 HDFS。
  NameNode 名称节点，是负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。 你可以把它理解成大管家，它不负责存储具体的数据。
 FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作 注意，这个两个都是文件，也会加载解析到内存中。  职责：
 管理 HDFS 的名称空间
 管理数据块（Block）映射信息
 配置副本策略
 处理客户端读写请求。
  SendaryNamenode NameNode节点的备份，它会定期的和namenode就行通信来完成整个的备份操作。并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。
职责：
 辅助 NameNode，分担其工作量。
 定期合并 fsimage和fsedits，并推送给NameNode。
 在紧急情况下，可辅助恢复 NameNode。
  DataNode 数据节点，用来具体的存储文件，维护了blockId 与 datanode本地文件的映射。 需要不断的与namenode节点通信，来告知其自己的信息，方便nameode来管控整个系统。
数据节点存储数据以块为单位，块大小默认是128m，默认会存储多份。
职责：
 存储实际的数据块。 执行数据块的读/写操作。  Hadoop集群节点组成 Hadoop集群逻辑节点组成 包括HDFS集群和YARN框架集群：
 HDFS集群  NameNode
SecondNameNode
DataNode
 YARN框架集群  ResourceManageNode
NodeManageNode
Hadoop集群物理节点应该如何分配？ 例如一套5节点的Hadoop集群：
节点1部署NameNode和ResourceManageNode（一般NameNode和ResourceManageNode部署在同一个节点上）这个节点为主节点。client node也在这个节点上；
节点2，3，4，5部署DataNode和NodeManageNode；
节点2部署SecondNameNode。
EMR集群介绍 EMR是AWS提供的Hadoop集群服务。现在使用云服务是主流趋势，这里介绍一下EMR集群节点组成。
 Master node  该节点管理集群，它通过运行软件组件来协调在其他节点之间分配数据和任务的过程以便进行处理。主节点跟踪任务的状态并监控集群的健康状况。每个集群具有一个主节点，并且可以创建仅包含主节点的单节点集群。
 Core node  该节点具有运行任务并在集群上的 Hadoop 分布式文件系统 (HDFS) 中存储数据的软件组件。多节点集群至少具有一个核心节点。
 Task node  该节点具有仅运行任务但不在 HDFS 中存储数据的软件组件。任务节点是可选的。
问题  Hadoop和MapReduce是什么关系？  我在搜索Hadoop和MapReduce的相关资料时会遇到到底搜“Hadoop”关键字还是“MapReduce”关键字。现在网上的博客很多也没有区分两者关系。
狭义Hadoop：本文中我们提到的Hadoop都算狭义Hadoop，MapReduce是一种运算框架，Hadoop包含MapReduce。
广义Hadoop：指Hadoop生态圈，其包括Hadoop、Hadoop Yarn、Hadoop MapReduce、Hadoop HDFS、Hive、Hbase、ZooKeeper、Spark等。
 参考文章
https://blog.csdn.net/aijiudu/article/details/72353510
https://blog.csdn.net/qianbing11/article/details/82357033
https://blog.csdn.net/xiaoshunzi111/article/details/48810239
https://blog.csdn.net/weixin_38750084/article/details/82963235
</content>
    </entry>
    
     <entry>
        <title>solr suggest实战</title>
        <url>https://jasonqian10.github.io/post/solr-suggest/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>lucene</tag><tag>suggest</tag>
        </tags>
        <content type="html"> solr suggest模块介绍 自动提示功能在现今的互联网产品中的应用几乎遍地都是。比如在京东主页搜索”苹果“，输入框下面就会自动联想出与苹果相关的关键词。
本文我们主要就来讲一下solr是如何去实现这样的自动提示功能。
Solr 中是通过 SuggestComponent 模块 为用户提供查询关键字的自动提示功能。可以使用此功能在搜索应用程序中实现强大的自动提示功能。
如何使用suggest模块 官方使用方式 先来介绍一下官方给出的使用方式，在solrconfig.xml中添加如下配置
&amp;lt;requestHandler name=&amp;#34;/suggest&amp;#34; class=&amp;#34;solr.SearchHandler&amp;#34; startup=&amp;#34;lazy&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;defaults&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;suggest&amp;#34;&amp;gt;true&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggest.count&amp;#34;&amp;gt;10&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;arr name=&amp;#34;components&amp;#34;&amp;gt; &amp;lt;str&amp;gt;suggest&amp;lt;/str&amp;gt; &amp;lt;!--这里配置的就是下面的searchComponent “suggest”--&amp;gt; &amp;lt;/arr&amp;gt; &amp;lt;/requestHandler&amp;gt;&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;FuzzyLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;DocumentDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;cat&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;weightField&amp;#34;&amp;gt;price&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;string&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt; 配置是什么作用 与是组合使用的，requestHandler定义的name &amp;ldquo;suggest&amp;rdquo;表示请求handler，当请求URL中出现&amp;rdquo;suggest&amp;rdquo;时，例如http://localhost:8983/solr/techproducts/suggest ，就会使用如上配置，进入solr.SearchHandler类，然后进入searchComponent配置的solr.SuggestComponent类。
searchComponent配置中几个重要的参数含义
 name：suggester名。可以任意取。当searchComponent中配置多个suggester时，可以通过查询参数 suggest.dictionary=mySuggester 指定查询哪一个suggester。且在response中会根据suggester名显示结果。 lookupImpl：查找实现。query词典和build词典的逻辑由其完成。solr提供了多种实现类。 dictionaryImpl：字典实现。词典中各字段值是由其获取的。solr提供了多种实现类。 field：指定主索引中的字段，需要stored，这个字段值用于生成suggest词典。 weightField：排序字段，需要数字类型，suggest结果会根据这个字段数值进行排序展示。 suggestAnalyzerFieldType：分析字段。lookupImpl类query和build阶段进行分析。 buildOnStartup：是否solr启动时构建suggest词典。建议false，否则启动很耗性能，可以通过suggest.build=true手动触发。 buildOnCommit和buildOnOptimize：是否在soft-commit和optimize后重建词典。可以根据自己soft-commit和optimize的频率来决定是否启用。如果频率比较高，建议false。可以通过suggest.build=true手动触发。  如何定制 上面讲的是官方给出的使用方式，当然使用上面的配置，能实现基本的suggest功能。下面讲讲我的项目中是如何定制suggest的。下面分别介绍两个suggest例子：
关键字自动提示 需求：用户输入关键字，前缀匹配出相关性结果，根据词频大小排序展示给用户。
我们把用户的查询关键字清洗出来，索引关键字和词频，用这份主索引生成词典。查询suggest词典，根据词频大小排序展示给用户。demo效果如下
searchComponent配置
&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34; enable=&amp;#34;true&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;AnalyzingInfixLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;indexPath&amp;#34;&amp;gt;index_keyword&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;highlight&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;DocumentDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;keyword&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;weightField&amp;#34;&amp;gt;tf&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;sortField&amp;#34;&amp;gt;tf&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;text_suggest&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnCommit&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnOptimize&amp;#34;&amp;gt;true&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt;  text_suggest参考官网配置
&amp;lt;fieldType name=&amp;#34;text_suggest&amp;#34; class=&amp;#34;solr.TextField&amp;#34; sortMissingLast=&amp;#34;true&amp;#34; omitTermFreqAndPositions=&amp;#34;true&amp;#34;&amp;gt; &amp;lt;analyzer&amp;gt; &amp;lt;tokenizer class=&amp;#34;solr.KeywordTokenizerFactory&amp;#34;/&amp;gt; &amp;lt;filter class=&amp;#34;solr.LowerCaseFilterFactory&amp;#34; /&amp;gt; &amp;lt;/analyzer&amp;gt; &amp;lt;/fieldType&amp;gt;  这里KeywordTokenizerFactory不会进行分词，会把输入文本作为完整的关键字进行索引。
为何选择AnalyzingInfixLookupFactory和DocumentDictionaryFactory的组合？
 AnalyzingInfixLookupFactory  主要在于AnalyzingLookupFactory与AnalyzingInfixLookupFactory区别。两者都是支持字段分析，后者优势在于支持前缀匹配（prefix matches），例如词典中存在关键字apple，当搜索app就能匹配到关键字apple。我的应用场景需要支持前缀匹配。
AnalyzingInfixLookupFactory支持indexPath配置，其他lookup不支持。
 DocumentDictionaryFactory  这个字典实现支持weightField配置，我们的应用场景需要根据关键字的词频进行推荐排序，正好可以通过weightField实现。
公司名自动提示 需求：本例子与上面的关键字提示有点不同，上面的例子是前缀匹配，本例需要短语匹配，只要命中公司中任意部分都会匹配上。根据文档频率排序展示。demo类似于天眼查搜索
searchComponent配置
&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;com.test.AnalyzingInfixPhraseQueryLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;HighFrequencyDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;indexPath&amp;#34;&amp;gt;index_company&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;COMPANY&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;suggest_text&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;highlight&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;int name=&amp;#34;minPrefixChars&amp;#34;&amp;gt;100&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnCommit&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt;  AnalyzingInfixPhraseQueryLookupFactory：这是个定制类，目的是为了实现短语查询
 suggest_text为
&amp;lt;fieldType name=&amp;#34;suggest_text&amp;#34; class=&amp;#34;solr.TextField&amp;#34; positionIncrementGap=&amp;#34;100&amp;#34;&amp;gt; &amp;lt;analyzer&amp;gt; &amp;lt;tokenizer class=&amp;#34;solr.StandardTokenizerFactory&amp;#34; /&amp;gt; &amp;lt;filter class=&amp;#34;solr.ICUTransformFilterFactory&amp;#34; id=&amp;#34;Traditional-Simplified&amp;#34;/&amp;gt; &amp;lt;filter class=&amp;#34;solr.LowerCaseFilterFactory&amp;#34; /&amp;gt; &amp;lt;/analyzer&amp;gt; &amp;lt;/fieldType&amp;gt;  这里会进行分词处理。查询时通过PhraseQuery进行搜索。
 HighFrequencyDictionaryFactory：这个词典类作用是能获取term的docFreq，把docFreq作为weight字段的值，我们的需求需要根据公司名的docFreq来排序展示。  suggest源码分析 suggest query源码执行时序图 suggest build源码执行时序图 suggest索引是在主索引基础上build出来的（通过HTTP请求参数suggest.build=true），build时序图即是suggest构建索引的过程。
suggest模块几个重要的类  SolrSuggester  solr初始化的时候，SolrSuggester类会维护lookupImpl和dictionaryImpl对象，query和build请求都会先经过SolrSuggester类。主要两个函数
a).getSuggestions()：query请求会走这个函数，调用lookupImpl对象的lookup()
b).build()：build请求走这个函数，调用lookupImpl对象的build()
 lookupImpl  lookupImpl类都是继承Lookup类，实现lookup()和build()。其中build()会调用dictionaryImpl对象的next()
 dictionaryImpl  dictionaryImpl类实现Dictionary接口，对象内部维护迭代器，迭代器实现InputIterator接口的next()，遍历获取所需字段。
</content>
    </entry>
    
     <entry>
        <title>solr搜索原理解析</title>
        <url>https://jasonqian10.github.io/post/solr-lucene-principle/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>lucene</tag>
        </tags>
        <content type="html"> solr与Lucene的关系 讲搜索流程之前先介绍一下solr与Lucene的关系。
Lucene是一个索引与搜索类库，而不是完整的程序。使用Lucene的方式主要有二种：一是自己编写程序，调用类库；二是使用第三方基于Lucene编写的程序，如下面介绍的Solr等。
Solr 是一个开源的搜索服务器，Solr 使用 Java 语言开发，主要基于 HTTP 和 Apache Lucene 实现。Solr是在Lucene上封装的完善的搜索引擎。
solr是门户，lucene是底层基础。通俗地说，如果Solr是汽车，那么Lucene就是发动机，没有发动机，汽车就没法运转，但对于用户来说只可开车，不能开发动机。
solr搜索流程分solr部分和Lucene部分，整体流程是请求先经过solr部分再进入Lucene部分。  说明 对应网上已有的素材或者文字符合作者想要的描述，就直接引用了，不重复造轮子
参考 https://www.cnblogs.com/forfuture1978/archive/2010/04/04/1704282.html
https://www.cnblogs.com/davidwang456/p/10570935.html https://blog.csdn.net/huangzhilin2015/article/details/89372127
 solr的启动过程 Solr可以独立运行，运行在Jetty中，Jetty 是一个开源的servlet容器，它为基于Java的web容器，其工作流程（也就是solr server启动过程）如下 这属于servlet范畴，本文不重点讨论，大家知道大体流程即可。
solr query流程 solr处理query的入口是SolrDispatchFilter，其实现了javax.servlet的Filter的接口。通过拦截servlet请求的方式进入solr处理。 上图可以看出solr中的流程。下面按照处理顺序重点讲一下solr几个处理query的核心类：
SearchHandler 真正处理请求的入口函数在SearchHandler.handleRequestBody()。对于不同的Request-Handler(qt)，请求会进入不同的SearchHandler，这由solrconfig.xml中的配置决定。我们这里以 select 请求为例，请求会进入SearchHandler。handleRequestBody()中主要做的事情就是依次调用SearchComponent列表的prepare，process，post方法。  如何定制SearchHandler   当然可以定制自己的Request-Handler，继承SearchHandler或RequestHandlerBase都可以。然后在solrconfig.xml中标签中配置自己开发的类。例如自定义TestSearchHandler，url path为/test，则样例配置如下
&amp;lt;requestHandler name=&amp;#34;/test&amp;#34; class=&amp;#34;com.test.TestSearchHandler&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;defaults&amp;#34;&amp;gt; &amp;lt;int name=&amp;#34;timeAllowed&amp;#34;&amp;gt;11000&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;echoParams&amp;#34;&amp;gt;none&amp;lt;/str&amp;gt; &amp;lt;int name=&amp;#34;rows&amp;#34;&amp;gt;20&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;defType&amp;#34;&amp;gt;xxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;op&amp;#34;&amp;gt;AND&amp;lt;/str&amp;gt; &amp;lt;float name=&amp;#34;tie&amp;#34;&amp;gt;1&amp;lt;/float&amp;gt; &amp;lt;str name=&amp;#34;qf&amp;#34;&amp;gt;xxxxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;fl&amp;#34;&amp;gt;xxxxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;sort&amp;#34;&amp;gt;score desc,_version_ desc&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/requestHandler&amp;gt; SearchComponent 初始化的时候SearchHandler中注册了本SearchHandler对应的SearchComponent的列表。一般包含主要功能的QueryComponent、FacetComponent、HighlightComponent、DebugComponent等，这些Component类都是继承SearchComponent这个抽象类。
SearchComponent抽象类定义了三个阶段prepare，process，post，SearchHandler.handleRequestBody()中会遍历所有注册的SearchComponent，调用这三个阶段，完成各个SearchComponent中功能。
 如何定制SearchComponent   定制SearchComponent的方法与定制SearchHandler的方法类似。继承SearchComponent或者对应功能的SearchComponent（如QueryComponent、FacetComponent、HighlightComponent、DebugComponent等）。然后在solrconfig.xml中标签配置自己开发的类。样例配置如下
&amp;lt;searchComponent name=&amp;#34;test_componet&amp;#34; class=&amp;#34;com.test.solr.TestComponent&amp;#34;&amp;gt; xxxxxx 里面可以配置自定义的参数 &amp;lt;/searchComponent&amp;gt; QueryComponet 进入查询最核心的QueryComponent，solr查询请求功能都是在本类中完成。 &#43; QueryComponent.prepare()
根据参数defType(配置在solrconfig.xml中标签中，不配置的话默认是”lucene“)，初始化QParser以及Query。以defType=”lucene“为例，其初始化过程如下
1.QParser parser = QParser.getParser(rb.getQueryString(), defType, req) 2.LuceneQParserPlugin.createParser(qstr, localParams, req.getParams(), req) 3.new LuceneQParser(qstr, localParams, params, req); 4.Query query = LuceneQParser.parse()  补充：QParser类与QParserPlugin类 QParser类的作用是QParser.parse() 可以构造出Query对象，查询时必须使用的对象。QParser一般由QParserPlugin.createParser()创建。QParser类与QParserPlugin类都是抽象类，不同的defType参数，对应不同的实现类。以defType=”lucene“为例，其初始化过程如下
  QueryComponent.process()   process()主要调用SolrIndexSearcher.search()，主要工作都在search()中完成。
 SolrIndexSearcher类继承IndexSearcher，对InderSearch做了封装。最终调用IndexSearcher.search()函数。 这是Lucene处理query的入口。
 Lucene query流程 Lucene处理query的入口是IndexSearcher.search()。其调用流程如下 createNormalizedWeight 创建归一化weight的流程，包括
重写Query对象 代码为：query = rewrite(query)
这个rewrite()作用是将 2.3 节中QParse.parse()解析出的Query对象转换成Query对象树，这棵树很重要，从Query对象树——》Weight对象树——》Scorer对象树，一直贯穿整个索引过程。为什么需要rewrite？ 因为solr很多不同的查询类型，比如前缀查询和通配符查询，从本质上，任何的查询都可以视为对多个关键词的查询。整个重写过程是把从Lucene角度认为原始的、开销大的查询对象转变成一系列开销小的查询对象的一个过程。
举个例子，查询语句 Title:(car* or bike) ，其QParse.parse()解析出的Query对象结构为
BooleanQuery(Title:(car* or bike)) &#43; BooleanQuery(Title:(car* or bike)) &#43; PrefixQuery(Title:car*) &#43; TermQuery(Title:or) --这个BooleanClause后面流程会继续处理掉 &#43; TermQuery(Title:bike) rewrite Query之后为
BooleanQuery(Title:(car* or bike)) &#43; MultiTermQueryConstantScoreWrapper(Title:car*) &#43; PrefixQuery(Title:car*) &#43; TermQuery(Title:or) --这个BooleanClause后面流程会继续处理掉 &#43; TermQuery(Title:bike) 对于PrefixQuery和FuzzyQuery，这些查询语句由于特殊的语法，可能对应的不是一个词，而是多个词，因而他们都有rewriteMethod对象指向MultiTermQuery的Inner Class，表示对应多个词，在查询过程中会得到特殊处理。
创建weight 调用 Query.createWeight(Searcher) 创建weight，以3.1.1中BooleanQuery为例，代码为
BooleanQuery.createWeight(Searcher) ... return new BooleanWeight(searcher) BooleanWeight构造函数主要实现是递归遍历Query树，生成Weight树。遍历过程中叶子节点是TermQuery，其TermQuery.createWeight(Searcher) 返回return new TermWeight(searcher)对象。在TermWeight构造函数中，需要做几件事：
 获取Similarity类
this.similarity = searcher.getSimilarity(needsScores); Similarity类是Lucene的相似度类（用于计算文档分数），嵌套在Weight对象中，Weight对象嵌套在Query对象中。这样Query树中每个query节点都会构造自己的打分类。 &amp;gt; 补充： &amp;gt; searcher.getSimilarity() 获取的Similarity对象是在初始化solr时创建的，根据schema.xml中是否定义了Similarity类，如果定义了，则用用户自定义的Similarity类进行打分，如果没有自定义，则使用solr默认的打分类BM25Similarity。后面的文章会详细介绍如何自定义打分。
 计算idf
this.stats = similarity.computeWeight(collectionStats, termStats); 这里 this.stats 对象已经计算好idf值。
   Lucene打分使用TF-IDF的打分公式，idf是其中一项。这里不详细介绍。
 计算分数 代码为
float v = weight.getValueForNormalization(); float norm = getSimilarity(needsScores).queryNorm(v); weight.normalize(norm, 1.0f); 这步计算的TF-IDF打分公式中仅与搜索语句相关与文档无关的部分(即不依赖于查询结果)，每个query子对象分数都是一样。分数存在weight对象中，在最终打分时可以直接使用。因为与文档无关，无需收集文档时遍历每篇文档重复计算，这里计算好，后面重复使用。
收集(collect)文档和打分(score) 收集文档 文档收集是收集匹配query的文档集，这里重点介绍涉及的几个类。  文档收集器（Collector及其实现类） 如上图Collector类是文档收集器接口类，其getLeafCollector()方法来获取LeafCollector对象（段文档收集器）。 TopDocsCollector类是个抽象类，实现了Collector接口，是一个文档收集器基类。其有个TopScoreDocCollector#create方法用于创建文档收集器对象，这个创建出来的对象一般是TopScoreDocCollector的子类。 SimpleTopScoreDocCollector类，是TopScoreDocCollector的子类，也是其内部类，是简单查询文档收集器。 PagingTopScoreDocCollector类，是TopScoreDocCollector的子类，也是其内部类，是用于分页查询文档收集器。
 段文档收集器（LeafCollector类及其实现类） LeafCollector是段文档收集器接口，LeafCollector#collect 方法最终完成收集文档的工作。段文档收集器会被Collector#getLeafCollector 初始化，包装在文档收集器中。这里会用到其子类ScorerLeafCollector。
  介绍好了这几个类之后，开始讲收集文档的过程。首先还在2.3步的时候，SolrIndexSearcher.search()方法里会调用SolrIndexSearcher#getDocListNC方法，在该方法中调用
final TopDocsCollector topCollector = buildTopDocsCollector(len, cmd); SolrIndexSearcher#buildTopDocsCollector方法中调用
TopScoreDocCollector.create(weightedSort, len, searchAfter, fillFields, needScores, needScores); 注意：当query参数中存在sort字段是会用。这里以上面的为例。 TopFieldCollector.create(weightedSort, len, searchAfter, fillFields, needScores, needScores); 这个方法在上面介绍TopScoreDocCollector类的时候提过到。TopScoreDocCollector#create中会创建TopScoreDocCollector的子类。当简单查询时创建SimpleTopScoreDocCollector，分页查询时创建PagingTopScoreDocCollector。
1. new SimpleTopScoreDocCollector(numHits); 2. new PagingTopScoreDocCollector(numHits, after); 然后会接下去会调用IndexSearcher#search(List leaves, Weight weight, Collector collector)方法，该方法的处理逻辑是for循环每个段，执行
leafCollector = collector.getLeafCollector(ctx); 获取段文档收集器对象leafCollector，这里的collector就是上面创建的SimpleTopScoreDocCollector类对象。
创建Scorer树 Scorer类是Lucene实现打分的类，是个抽象类，每个Query子类都会集成Scorer类，作为每个Query打分的入口。如TermQuery的Scorer类是TermScorer，TermQuery打分的入口则是TermScorer#score()。
创建Scorer树入口代码：
BulkScorer scorer = weight.bulkScorer(ctx) 创建Scorer对象树的过程其实与创建Weight对象树的过程类似。遍历Weight树依次创建Scorer对象。以TermQuery对象为例，初始化关键代码：
1.Scorer scorer = TermWeight.scorer() 2.return new TermScorer(this, docs, similarity.simScorer(stats, context)); 创建TermScorer时传入了SimScorer对象，由Similarity#simScorer获取而来，Similarity类的对象，在创建TermQuery时维护在对象内部。
similarity.simScorer(stats, context) SimScorer类是Similarity类的内部类，具体打分逻辑就是SimScorer#score()方法中实现。TermScorer对象初始化时会在对象内部维护SimScorer对象。打分时调用顺序会是： TermScorer.score() ——》 SimScorer.score()
打分score 打分入口是BulkScorer#score()方法
scorer.score(leafCollector, ctx.reader().getLiveDocs()); 进入Weight#score()方法，这里会执行
collector.setScorer(scorer); 为收集器对象传入scorer打分对象（这个scorer对象会最终被LeafCollector#collect中使用来进行打分）。 调用Weight#scoreAll()方法，这里会调用DocIdSetIterator#nextDoc()方法，DocIdSetIterator是个迭代器，依次拿出匹配到的文档id。
int doc = iterator.nextDoc()  至于如何拿出这些匹配到文档id，过程比较复杂，涉及Lucene索引文件相关的原理，后面文章会细聊。
 这里需要注意，每个段拿出的文档id是基于本段的排序。而最终的文档id是需要全局唯一的，这里需要加上docBase。
final int docBase = context.docBase; pqTop.doc = doc &#43; docBase; 然后传入拿出的文档id，调用段文档收集器LeafCollector#collect 方法
collector.collect(doc); 然后进入 TermScorer.score() ——》 SimScorer.score()完成打分。
</content>
    </entry>
    
</search>