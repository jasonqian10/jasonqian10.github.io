<search>
    
     <entry>
        <title>【Java集合框架】看这一篇就够了</title>
        <url>https://jasonqian10.github.io/post/java/java-collection/</url>
        <categories>
          <category>java</category>
        </categories>
        <tags>
          <tag>java</tag><tag>集合框架</tag>
        </tags>
        <content type="html"> 概述 集合框架是Java编码中最常用的知识点，本篇主要梳理一下Java集合框架，总结出各个常用的集合知识点。
 Collection接口下有List、Set、Queue，三者都是接口 Map是独立接口（与Collection是平级关系），下有Hashtable，LinkedHashMap，HashMap，TreeMap List下有ArrayList，Vector，LinkedList Set下有HashSet，LinkedHashSet，TreeSet Queue下有阻塞队列和非阻塞队列；堵塞队列中又分单端堵塞队列和双端堵塞队列  Collection Collection家族图谱：
List ArrayList ArrayList是最常用的数据结构之一，主要技术点有
 使用可变大小的数组实现: transient Object[] elementData；默认容量10: private static final int DEFAULT_CAPACITY = 10; 每次扩容50%: int newCapacity = oldCapacity &#43; (oldCapacity &amp;gt;&amp;gt; 1); 它允许所有元素，包括多个null，允许重复元素 默认从末尾添加，保持添加顺序；remove时只会remove掉数组中第一个出现的元素 各方法时间复杂度  get(i) 直接读取第i个下标，复杂度 O(1) add(E) 添加元素，直接在后面添加，复杂度O(1) add(index, E) 添加元素，在第index个元素后面插入，后面的元素需要向后移动，复杂度O(n) remove()删除元素，后面的元素需要逐个移动，复杂度O(n)   总结
 优点: 底层数据结构是数组，查询快，增删慢（需数组间拷贝数据）。 缺点: 线程不安全，效率高  LinkedList LinkedList是最常用的数据结构之一，主要技术点有
 使用双链表，双向链表就是通过Node类来体现，Node对象会多消耗额外的内存（相对于ArrayList的缺点） 申请内存容量和实际size一致，不会额外申请容量（ArrayList申请的容量如果未使用，也会有内存分配） 它允许所有元素，包括多个null，允许重复元素 默认从末尾添加，保持添加顺序；remove时只会remove掉数组中第一个出现的元素 各方法时间复杂度  get(i) 获取第几个元素，依次遍历，复杂度O(n)。实际使用二分查找（只分一次），从中间分隔，如果i&amp;lt;一半size,则从头往后找；如果i&amp;gt;一半size,则从末尾向前找。 add(E) 添加到末尾，复杂度O(1) add(index, E) 添加第index个元素，需要先查找到第index个元素，直接指针指向操作，复杂度O(n)。实际使用二分查找（只分一次），同get(i) remove(index)删除元素，直接指针指向操作，remove操作的时间复杂度是O(1)，但是查找index位置时间复杂度同get(i) remove(Object)删除元素，直接指针指向操作，remove操作的时间复杂度是O(1), 但是查找Object位置的时间复杂度是O(n)，逻辑是从头开始循环知道找到第一个出现Object的位置为止。   总结
 优点: 底层数据结构是链表，查询慢，增删快。 缺点: 线程不安全，效率高  Vector 技术点
 使用可变大小的数组实现（同ArrayList），加synchronized锁，线程安全 默认初始化容量10（同ArrayList），扩容时如果未设置扩容系数则是双倍扩容，如果设置了扩容系数则按扩容系数扩容（这点与ArrayList不同，ArrayList每次扩容50%）。
int newCapacity = oldCapacity &#43; ((capacityIncrement &amp;gt; 0) ? capacityIncrement : oldCapacity); 它允许所有元素，包括多个null，允许重复元素(同ArrayList,LinkedList)
 默认从末尾添加，保持添加顺序；remove时只会remove掉数组中第一个出现的元素
 时间复杂度同ArrayList
  总结
 优点: 底层数据结构是数组，查询快，增删慢。 缺点: 线程安全，效率低  Set HashSet 技术点
 底层数据结构是由哈希表HashMap实现，HashSet的元素作为key, value用同一个new Object()表示 private static final Object PRESENT = new Object();，存入HashMap中。所以HashSet是对HashMap的简单包装，重点关注HashMap HashSet存储元素的顺序并不是按照存入时的顺序（和List不同），是按照HashCode值顺序来排序的。 无重复key，元素唯一；允许null（只有一个null，无重复） 依赖hashCode()和equals()判断key是否相等 add时HashSet中如果已经存在，则返回false，否则返回true  LinkedHashSet 技术点
 继承HashSet，底层数据结构是LinkedHashMap，链表和哈希表结构，链表保序（元素保持与添加顺序一致），哈希表保唯一（无重复元素），细节请看LinkedHashMap 元素唯一，不能重复；只能有一个null值；集合顺序保持与添加顺序一致  TreeSet 技术点
 底层数据结构是红黑树 元素有序，但是非添加元素的顺序，顺序为自然排序（比如元素为String，则按照字典排序）或者比较器排序（元素为自定义Object时，如果Object实现了Comparator接口，则以自定义的排序为准）  Queue 特点  队列是一种特殊的线性表，它只允许基本操作：在队列尾部加入一个元素，和从队列头部移除一个元素（以一种先进先出的方式管理数据） 队列按照是否线程安全可以分为两类：1) 线程不安全：这类主要是LinkedList, 其是实现了Queue接口的，严格意义上来讲算一种队列（非传统意义上的队列，一般讨论队列时不讨论它），其是线程不安全、非堵塞、双端队列； 2）线程安全：剩下的都是线程安全队列，java中并发队列都是在java.util.concurrent并发包下的，本节我们讨论的是这个范畴的队列。在线程安全的队列中，我们分为堵塞队列和非堵塞队列。 队列家族，Queue家族主要有如下接口   Queue:FIFO队列
 BlockingQueue:这是堵塞队列，接口。主要继承类有ArrayBlockingQueue、LinkedBlockingQueue ConcurrentLinkedQueue:这是非堵塞队列，是具体实现类 Deque:这个数据结构代表的是双向队列，队列的进出并不只是单向的。既然数据的操作形式变成了双向的，那操作方法的定义自然也要变为原来的两倍。  BlockingDeque:这是堵塞队列接口，实现类有LinkedBlockingDeque LinkedList:非堵塞队列，是具体实现类    堵塞队列 BlockingQueue     抛出异常 特殊值(false/null) 阻塞 超时     插入 add(e) offer(e) put(e) offer(e, time, unit)   移除 remove() poll() take() poll(time, unit)   检查 element() peek() 无 无    其中带有堵塞功能的接口使用可中断方法lock.lockInterruptibly()，如put(), offer(e, time, unit),take(),poll(time,unit)
ArrayBlockingQueue 技术点
 继承自AbstractBlockingQueue，实现BlockingQueue 底层是循环数组保存数据。循环数组：由于队列大小一旦初始化后就固定，当添加队列到数组尾部后，会重新回到头部继续添加(因为出栈是从头部位置依次移出，所以头部位置会空出，这样形成了循环数组) 创建时，必须要给它指定一个队列的大小。属于有界队列。 初始化时指定的容量，就是队列最大的容量，不会出现扩容，容量满，则阻塞进队操作；容量空，则阻塞出队操作 队列不支持空元素 线程安全队列，使用ReentrantLock锁，入栈出栈一把锁。takeIndex和putIndex分别记录出队和入队的数组下标边界。 主要方法：  构造函数:需要指定初始化队列的大小和是否公平堵塞；内部使用数组保存队列；构造函数会初始化ReentrantLock对象全局使用。 add(e):使用抽象类AbstractBlockingQueue的方法。实际会调用offer(e)，offer(e)返回true则返回true，否则抛异常。 offer(e):如果队列达到数组大小(初始化时设定)，则返回false；否则加入队列 put(e):先加锁，然后判断队列是否已经满，如满则堵塞线程，等待Condition notFull唤醒 offer(e,time,unit):队列满时则等待time时间，超时后则返回false。 remove():使用抽象类AbstractBlockingQueue的方法。实际会调用poll()，如果有数据则返回取出的数据，否是抛异常。 poll():有数据则返回数据，否则返回null。 take():有数据则返回数据，否则堵塞(Condition notEmpty.await())。 poll(time,unit):有数据则返回数据，否则堵塞time时间（notEmpty.awaitNanos(nanos)）。 enqueue(e):入栈函数。按照当前队列指针插入数据，然后会判断指针是否已经达到队列尾部，如是则指针重新指向0位置；同时队列当前元素个数自加1；唤醒Condition notEmpty堵塞线程（取队列时）。 dequeue:出栈函数。逻辑与enqueue相反。 element():使用抽象类AbstractBlockingQueue的方法。实际会调用peek()，peek()返回数据则直接返回，否则抛异常。 peek():查看当前队列头部元素(takeindex的位置)，如果当前为空队列，则返回null，否则返回当前队列头部元素。   LinkedBlockingQueue 技术点
 继承自AbstractBlockingQueue，实现BlockingQueue 底层使用单链表保存数据，链表元素数据结构是Node 创建队列时可以选择是否指定队列大小。它如果不指定容量，默认为Integer.MAX_VALUE，也就是无界队列；如果指定则以指定大小为准。为了避免队列过大造成机器负载或者内存爆满的情况出现，我们在使用的时候建议手动传一个队列的大小。默认是无界队列，指定大小后则是有界队列。 与ArrayBlockingQueue不同的是，LinkedBlockingQueue内部分别使用了takeLock 和 putLock 对并发进行控制，AtomicInteger count进行计数同步，也就是说，添加和删除操作并不是互斥操作，可以同时进行，这样也就可以大大提高吞吐量。入栈、出栈两把锁。 主要方法：  构造函数：支持指定参数和不指定参数两种初始化方式。初始化时定义last和head两个指针 last = head = new Node(null); ，last指针指定队列尾部(当前入栈位置)，head指针针对队列头部(当前出栈位置)。 enqueue(e):入栈函数。last = last.next = node; 主要逻辑就是last指针往后移动。 dequeue:出栈函数。去掉头部Node,head指针往后移动。   PriorityBlockingQueue 技术点
 继承自AbstractBlockingQueue，实现BlockingQueue。 底层使用数组保存数据 创建队列时可以选择指定队列大小（如果不指定默认初始队列大小11）；队列是优先级队列，需要初始化时要么指定comparator比较器，要么是队列元素实现了Comparable接口。 当添加队列大小达到初始队列长度后会自动扩容，理论上能无限扩容，属于无界阻塞队列  DelayQueue 技术点
 继承自AbstractBlockingQueue，实现BlockingQueue。队列元素需要实现Delayed接口（此接口集成Comparable接口） 内部使用PriorityQueue&amp;lt;E&amp;gt; q = new PriorityQueue&amp;lt;E&amp;gt;();存放队列。 取队列元素时，会调用队列元素的Delayed#getDelay()方法，来判断是否延迟到期，是则可以取出，否则返回null。  BlockingDeque 双端堵塞队列，支持从队头、队尾两头插入和取出。
这类队列原理与单端队列类似，这里不在详细介绍。
非堵塞队列 ConcurrentLinkedQueue 技术点
 基于链表实现的无界线程安全队列。使用CAS算法保证线程安全。不阻塞 无论入队还是出队，都是在死循环中进行的，也就是说，当一个线程调用了入队、出队操作时，会尝试获取链表的tail、head结点进行插入和删除操作，而插入和删除是通过CAS操作实现的，而CAS具有原子性。故此，如果有其他任何一个线程成功执行了插入、删除都会改变tail/head结点，那么当前线程的插入和删除操作就会失败，则通过循环再次定位tail、head结点位置进行插入、删除，直到成功为止。  Map HashMap 技术点
 存储元素的顺序并不是按照存入时的顺序，是按照hash(HashCode)值顺序来排序的；无重复元素，元素唯一；允许null（只有一个null，无重复）
 时间复杂度
 put方法：存数据的时间复杂度是O(1)，因为只需要首先根据Key计算哈希值，其实就是数组的下标，找到存放的位置，然后把key-value链表节点链接上去就OK了。 get方法：取数据的时间复杂度最好是O(1)，最差是O(n)。根据根据key计算哈希地址，得到key存储的数组下标，如果该位置就只有一个节点或者要找的节点就在表头，那么就不用再往后遍历链表了，所以最好情况下时间复杂度是O(1)；如果要找到的节点在链表表尾，那么就需要一个一个遍历链表，所以此时最差情况下为O(n)。所以get方法的时间复杂度为O(1)~ O(n)。  底层数据结构：数组＋链表＋红黑树。可以理解为Key的存储是数组，Value存储是链表&#43;红黑树
 数组：transient Node[] table，这是一个Node类型的数组（也有称作Hash桶），HashMap中每个key值对应数组中一个元素。默认初始化数组大小为16 static final int DEFAULT_INITIAL_CAPACITY = 1 &amp;lt;&amp;lt; 4
 链表：每个节点(key)就是Node ，Node结构是个单链表。无hash冲突时链表长度就是1，当hash冲突时新增的Value值就添加到链表尾部(jdk1.7时添加在首部)。 红黑树：链表长度大于8的时候转换为红黑树。  查询元素
 通过 hash(key) 计算出key的hashcode值，然后散列到数组范围内（为了均匀分布table数据和充分利用空间） 所以在查询上的访问速度比较快。   注意：HashMap.get(key) 如果不存在，会返回null，注意判空
 添加元素
  以put()为例：
1）判断是否为空table，如果是空table则先进行扩容resize()；
2）根据hash(key)计算出应该添加到table中哪个节点；
3）判断此节点是否为空，为空则直接插入；否则
4）判断待插入的key和此节点已存的key是否完全一致（==、hashcode、equals都一样），如果相等则替换老的元素；否则
5）说明是hash碰撞。先判断当前的节点p是否为红黑树，如果是则存入红黑树，否则
6）以链表的形式保存，把当前传进来的参数生成一个新的节点保存在链表的尾部（JDK1.7保存在首部）
7）添加完链表后判断是否长度超过8，超过则会把链表转成红黑树
8）添加完待插入的数据后，判断是否达到扩容阀值，如果达到，则还需进行扩容（2倍）
 加载因子loadFactor  加载因子初始化时可以指定，默认是0.75f static final float DEFAULT_LOAD_FACTOR = 0.75f;, 不建议修改。表示Hsah表中元素的填满的程度，若加载因子越大，填满的元素越多，好处是空间利用率高了，但冲突的机会加大了；反之，加载因子越小，填满的元素越少，好处是冲突的机会减小了，但空间浪费多了。
 扩容机制  HashMap扩容可以分为三种情况：
第一种：使用默认构造方法初始化HashMap。开始初始化的时候会返回一个空的table，并且thershold为0。在第一次put的时候，会先扩容，容量为默认值DEFAULT_INITIAL_CAPACITY也就是16。同时threshold = DEFAULT_INITIAL_CAPACITY * DEFAULT_LOAD_FACTOR = 12。（这里threshold扩容阀值）
第二种：指定初始容量的构造方法初始化HashMap，开始初始化的时候会返回一个空的table，threshold设置为最接近初始容量的2^n（比如设置初始容量initialCapacity=13，则初始threshold=16=2^4）。第一次put的时候，会先扩容，扩容容量为newCap=threshold，然后设置threshold = 当前的容量（newCap） * DEFAULT_LOAD_FACTOR。
 举例：new HashMap时传入capacity=13
&amp;ndash;》初始化时 threshold=16，此时Node[] table = null
&amp;ndash;》第一次put时，先扩容capacity=threshold=16，初始化Node[] newTab = (Node[])new Node[16]，threshold=capacity * DEFAULT_LOAD_FACTOR = 16*0.75=12
 第三种：HashMap不是第一次扩容。如果HashMap已经扩容过的话，那么每次table的容量以及threshold量为原有的两倍。
 这边引申到一个问题HashMap是先插入还是先扩容?
HashMap初始化后首次插入数据时，先发生resize扩容再插入数据，之后每当插入的数据个数达到threshold时就会发生resize，此时是先插入数据再resize
 LinkedHashMap 技术点
 数据结构：HashMap &#43; LinkedList
 继承HashMap，沿用HashMap的方法来操作数据结构 也用LinkedList维护插入元素的先后顺序，每个节点都进行了双向的连接，维持插入的顺序（默认）。head指向第一个插入的节点，tail指向最后一个节点。  key和value都允许为空；key重复会覆盖,value可以重复；以插入顺序有序排列；LinkedHashMap是非线程安全的
 时间复杂度
 构造器
 新增accessOrder变量，该属性指取得键值对的方式,是个布尔值,false表示插入顺序,true表示访问顺序（也就是访问次数，插入和访问都会将当前节点放置到尾部，尾部代表的是最近访问的数据） private final boolean accessOrder;, 默认都是采用插入顺序来维持取出键值对的次序.所有的构造方法都是通过父类的构造方法来建造对象的。
 LinkedHashMap是HashMap的亲儿子，直接继承HashMap类。LinkedHashMap中的节点元素为Entry，直接继承HashMap.Node，再此基础上新增Entry&amp;lt;K,V&amp;gt; before, after; 实现双链表，维护顺序关系。 在这里需要明确Entry对象内的next,before,after这三个属性的意思: next是用于维护HashMap指定table位置上连接的Entry顺序（即原HashMap链表顺序）;before、after是用于维护Entry插入的先后顺序的。
 新增LinkedHashMap.Entry head, tail，用来维护节点间的双链表表头和表尾。transient LinkedHashMap.Entry&amp;lt;K,V&amp;gt; head;,transient LinkedHashMap.Entry&amp;lt;K,V&amp;gt; tail;head、tail、before、after会把所有的Entry连成完整的双链表，来维护插入的顺序（默认）。
   用个例子简单理解一下： 插入顺序 实际的链表插入顺序关系：  查询元素
  LinkedHashMap中的get方法与父类HashMap处理逻辑一样；新增逻辑是如果查找到节点，则判断accessOrder=true，是则返回值之前，将该节点移动到对应桶中链表的尾部。（通过覆写afterNodeAccess()函数实现）
public V get(Object key) { Node e; if ((e = getNode(hash(key), key)) == null) return null; if (accessOrder) afterNodeAccess(e); return e.value; }  添加元素  put()方法中覆写Node&amp;lt;K,V&amp;gt; newNode方法，创建新节点的时候时把新增的entry添加到双向链表的尾部。其他逻辑沿用HashMap。
TreeMap 技术点
 支持排序，可以初始化的时候设置自定义比较器Comparator，如果未设置自定义的比较器则使用key的默认比较器 TreeMap实现了红黑树的结构，形成了一颗二叉树 不允许重复key, 不允许key=null 无序集合，不保持插入顺序 遍历逻辑put、get: private transient Entry&amp;lt;K,V&amp;gt; root; 全局变量表示树的根节点，每个遍历从根节点出发，进行查找  HashTable 技术点
 不允许key=null，使用synchronized实现线程安全 其他基本上与HashMap一致  常见问题汇总 ArrayList和LinkedList的区别？  ArrayList是实现了基于动态数组的数据结构，LinkedList基于链表的数据结构。 对于随机访问get和set，ArrayList觉得优于LinkedList，因为LinkedList要移动指针。 对于新增和删除操作add和remove，LinedList比较占优势，因为ArrayList要移动数据。  JDK1.8前后 HashMap的变动，以及为何有这变动？ HashMap在jdk1.8之前结构为数组&#43;链表，缺点就是哈希函数很难使元素百分百的均匀分布，这会产生一种极端的可能，就是大量的元素存在一个桶里，此时的时间复杂度为O(n)，极大的放慢了计算速率。
在jdk1.8之后，HashMap采用数组加链表或是红黑树的形式：
 在HashMap添加元素时，按照数组&#43;链表形式添加，当桶中的数量大于8时，链表会转换成红黑树的形式。 删除元素、扩容时，同上，数量大于8时，也是采用红黑树形式存贮，但是在数量较少时，即数量小于6时，会将红黑树转换回链表。 遍历、查找时，使用红黑树，他的时间复杂度O(log n)，便于性能的提高。  HashMap(1)、LinkedHashMap(2)、TreeMap(3)比较  key=null: 1,2支持，3不支持
 内部存储节点不同
  ​ 1.HashMap
HashMap.Node&amp;lt;K,V&amp;gt; implements Map.Entry&amp;lt;K,V&amp;gt; final int hash; final K key; V value; Node&amp;lt;K,V&amp;gt; next; //主要为了存储hash冲突的node ​ 2.LinkedHashMap
LinkedHashMap.Entry&amp;lt;K,V&amp;gt; extends HashMap.Node&amp;lt;K,V&amp;gt; Entry&amp;lt;K,V&amp;gt; before, after; //在1的基础上增加,记录添加顺序  transient LinkedHashMap.Entry&amp;lt;K,V&amp;gt; head,tail; //LinkedHashMap全局的变量，非Entry内部的变量）增加头尾Entry记录 ​ 3: TreeMap
TreeMap.Entry&amp;lt;K,V&amp;gt; implements Map.Entry&amp;lt;K,V&amp;gt; //典型红黑树结构  K key; V value; Entry&amp;lt;K,V&amp;gt; left; Entry&amp;lt;K,V&amp;gt; right; Entry&amp;lt;K,V&amp;gt; parent; boolean color = BLACK;  集合的顺序  1是无序集合，以key的Hash值进行排序 2是有序集合，以元素添加的顺序（或者元素被访问的顺序）排序 3是无序集合，以自定义比较器或者自然比较排序     注：这里的&amp;rdquo;有序&amp;rdquo;、&amp;rdquo;无序&amp;rdquo;是指相对于元素被添加的顺序而言的。
 ArrayBlockingQueue单锁与LinkedBlockingQueue双锁? 主要是数据结构不同。
对于数组(ArrayBlockingQueue)：添加、移除是修改同一个array，需要互斥保护。
对于单链表(LinkedBlockingQueue)：添加、移除其实只是修改头、尾两个Node，无需互斥，可以双锁分别保证头部修改互斥、尾部修改互斥即可。
集合工具类 。。。
</content>
    </entry>
    
     <entry>
        <title>【java8】(三)stream 详细用法</title>
        <url>https://jasonqian10.github.io/post/java/java8-stream/</url>
        <categories>
          <category>java</category>
        </categories>
        <tags>
          <tag>java8</tag><tag>stream</tag>
        </tags>
        <content type="html"> 概述 Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。
特点  不是数据结构，不会保存数据
 不会修改原来的数据源，它会将操作后的数据保存到另外一个对象中。（补充：peek方法例外）
 惰性求值，流在中间处理过程中，只是对操作进行了记录，并不会立即执行，需要等到执行终止操作的时候才会进行实际的计算
  分类   Stream操作分类   中间操作(Intermediate operation) 无状态 map (mapToInt, flatMap 等)、filter、peek、parallel、sequential、unordered   有状态 distinct、sorted、limit、skip   结束操作(Terminal operation) 非短路操作 forEach、forEachOrdered、toArray、reduce、collect、min、max、count、iterator   短路操作 anyMatch、 allMatch、 noneMatch、 findFirst、 findAny、 limit    无状态：指元素的处理不受之前元素的影响 有状态：指该操作只有拿到所有元素之后才能继续下去 非短路操作：指必须处理所有元素才能得到最终结果 短路操作：指遇到某些符合条件的元素就可以得到最终结果，如 A || B，只要A为true，则无需判断B的结果  pipeline stream处理过程就是一个pipeline，中间操作（filter、sorted、map）在遇到结束操作（collect）之前是不会实际执行的。每个步骤是有序执行下去。
这是以Stream&amp;lt;Dish&amp;gt;为例，白盒描述每步骤中间的处理结果是什么样的（关注每步操作后的数据类型）。
stream和parallelStream区别  stream是单管道，parallelStream是多管道（并行），从性能上来看parallelStream效率更高
 parallelStream提供了更简单的并发执行的实现，但并不意味着更高的性能，它是使用要根据具体的应用场景。如果cpu资源紧张parallelStream不会带来性能提升；如果存在频繁的线程切换反而会降低性能
 任务之间最好是状态无关的，因为parallelStream默认是非线程安全的，可能带来结果的不确定性
 stream.parallel()是并行，线程不安全的。两种方式可以避免造成不安全的后果：
   1)stream.parallel().forEach() 如果使用forEach，那么可以通过创建线程安全的集合Collections.synchronizedList(new ArrayList&amp;lt;&amp;gt;()) 来避免
2)使用collect、reduce等stream的结束操作，收集所有元素到新集合是线程安全的
 具体用法 stream创建 //1. Arrays.stream() 入参是数组 String [] stringArray = {&amp;#34;aaa&amp;#34;, &amp;#34;bbb&amp;#34;, &amp;#34;ccc&amp;#34;}; Arrays.stream(stringArray).forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //2. Collection.stream() 集合的stream方法，Collection的子集List/Set等都支持 List&amp;lt;String&amp;gt; stringList = Arrays.asList(&amp;#34;aaa&amp;#34;, &amp;#34;bbb&amp;#34;, &amp;#34;ccc&amp;#34;); stringList.stream().forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //3. Stream.of() Stream类静态方法 Stream.of(&amp;#34;aaa&amp;#34;, &amp;#34;bbb&amp;#34;, &amp;#34;ccc&amp;#34;).forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //4. Stream.builder().build() Stream类build模式创建Stream Stream.builder().add(&amp;#34;abc&amp;#34;).add(&amp;#34;bbb&amp;#34;).build().forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); 这四种是根据具体的数组或者集合对象，创建的流，在创建流之前，这些对象的大小（长度）已经确认，所以这个种方式的流，也被成为有限流
//5. Stream.iterate() 参数为：初始值，Function函数式接口对象。是无限流，需要limit()限制 //创建一个从0开始，每次迭代增加2的流 Stream.iterate(0, n -&amp;gt; n&#43;2).limit(10).forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //6. Stream.generate() 参数：Supplier函数式接口对象。 是无限流，需要limit()限制 Stream.generate(Math::random).limit(10).forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); 5, 6两种是Stream自带的静态方法，创建的Stream流是无限流，需要limit()指定大小。
//7. BufferedReader.lines() 从文件中直接读出Stream流 BufferedReader reader = new BufferedReader(new FileReader(&amp;#34;/Users/jason/work/code/my-project/my-learning/src/main/resources/test_stream.txt&amp;#34;)); Stream&amp;lt;String&amp;gt; lineStream = reader.lines(); lineStream.forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //8. Pattern.splitAsStream() 将字符串分隔成流 Pattern pattern = Pattern.compile(&amp;#34;,&amp;#34;); Stream&amp;lt;String&amp;gt; stringStream = pattern.splitAsStream(&amp;#34;a,b,c,d&amp;#34;); stringStream.forEach(System.out::println); 这两种主要是从其他形式的对象，经过处理转化成Stream流。用的会少一些。
综合来看，1、2、3种创建流方法在我们平时开发中较为常用。看过本文后，大家可以向4-8扩展使用。
流的中间操作 //1. 筛选与切片 // filter：过滤流中的某些元素 // limit(n)：获取n个元素 // skip(n)：跳过n元素，配合limit(n)可实现分页 // distinct：通过流中元素的 hashCode() 和 equals() 去除重复元素 Stream&amp;lt;Integer&amp;gt; stream = Stream.of(6, 4, 6, 7, 3, 9, 8, 10, 12, 14, 14); Stream&amp;lt;Integer&amp;gt; newStream = stream.filter(s -&amp;gt; s &amp;gt; 5) //6 6 7 9 8 10 12 14 14  .distinct() //6 7 9 8 10 12 14  .skip(2) //9 8 10 12 14  .limit(2); //9 8 newStream.forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //2. map &amp;amp; flatMap List&amp;lt;String&amp;gt; list = Arrays.asList(&amp;#34;a,b,c&amp;#34;, &amp;#34;1,2,3&amp;#34;); //2.1 map: 将每个元素转成一个新的且不带逗号的元素 Stream&amp;lt;String&amp;gt; s1 = list.stream() .map(s -&amp;gt; s.replaceAll(&amp;#34;,&amp;#34;, &amp;#34;&amp;#34;)); s1.forEach(System.out::println); // abc 123  //2.2 flatMap Stream&amp;lt;String&amp;gt; s3 = list.stream().flatMap(s -&amp;gt; { //将每个元素转换成一个stream  String[] split = s.split(&amp;#34;,&amp;#34;); Stream&amp;lt;String&amp;gt; s2 = Arrays.stream(split); return s2; }); s3.forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //3. 排序 //3.1 sorted()：自然排序，流中元素需实现Comparable接口 List&amp;lt;String&amp;gt; list2 = Arrays.asList(&amp;#34;aa&amp;#34;, &amp;#34;ff&amp;#34;, &amp;#34;dd&amp;#34;); //String 类自身已实现Compareable接口 list2.stream().sorted().forEach(System.out::println);// aa dd ff  //3.2 sorted(Comparator com)：定制排序，自定义Comparator排序器 Student student1 = new Student(&amp;#34;aa&amp;#34;, 10); Student student2 = new Student(&amp;#34;bb&amp;#34;, 20); Student student3 = new Student(&amp;#34;aa&amp;#34;, 30); Student student4 = new Student(&amp;#34;dd&amp;#34;, 40); List&amp;lt;Student&amp;gt; studentList = Arrays.asList(student1, student2, student3, student4); //自定义排序：先按姓名升序，姓名相同则按年龄升序 studentList.stream().sorted( (o1, o2) -&amp;gt; { if (o1.getName().equals(o2.getName())) { return o1.getAge() - o2.getAge(); } else { return o1.getName().compareTo(o2.getName()); } } ).forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); //4. peek(), 如同于map，能得到流中的每一个元素。但map接收的是一个Function表达式，有返回值；而peek接收的是Consumer表达式，没有返回值。 Student student5 = new Student(&amp;#34;aa&amp;#34;, 10); Student student6 = new Student(&amp;#34;bb&amp;#34;, 20); List&amp;lt;Student&amp;gt; studentList2 = Arrays.asList(student5, student6); studentList2.stream() .peek(o -&amp;gt; o.setAge(100)) .forEach(System.out::println); System.out.println(&amp;#34;----------------&amp;#34;); 流的结束操作 /** * 1. 匹配、聚合操作 * allMatch：接收一个 Predicate 函数，当流中每个元素都符合该断言时才返回true，否则返回false * noneMatch：接收一个 Predicate 函数，当流中每个元素都不符合该断言时才返回true，否则返回false * anyMatch：接收一个 Predicate 函数，只要流中有一个元素满足该断言则返回true，否则返回false * findFirst：返回流中第一个元素 * findAny：返回流中的任意元素 * count：返回流中元素的总个数 * max：返回流中元素最大值 * min：返回流中元素最小值 */ List&amp;lt;Integer&amp;gt; listInterger = Arrays.asList(1, 2, 3, 4, 5); boolean allMatch = listInterger.stream().allMatch(e -&amp;gt; e &amp;gt; 10); //false boolean noneMatch = listInterger.stream().noneMatch(e -&amp;gt; e &amp;gt; 10); //true boolean anyMatch = listInterger.stream().anyMatch(e -&amp;gt; e &amp;gt; 4); //true  Integer findFirst = listInterger.stream().findFirst().get(); //1 Integer findAny = listInterger.stream().findAny().get(); //1  long count = listInterger.stream().count(); //5 Integer max = listInterger.stream().max(Integer::compareTo).get(); //5 Integer min = listInterger.stream().min(Integer::compareTo).get(); //1//2. collect操作：接收一个Collector实例，将流中元素收集成另外一个数据结构。Collectors类有众多静态方法来生成Collector实例。 Student s11 = new Student(&amp;#34;aa&amp;#34;, 10); Student s21 = new Student(&amp;#34;bb&amp;#34;, 20); Student s31 = new Student(&amp;#34;cc&amp;#34;, 10); List&amp;lt;Student&amp;gt; list3 = Arrays.asList(s11, s21, s31); //Collectors.toList() 装成list List&amp;lt;Integer&amp;gt; ageList = list3.stream().map(Student::getAge).collect(Collectors.toList()); // [10, 20, 10]  //Collectors.toSet() 转成set Set&amp;lt;Integer&amp;gt; ageSet = list3.stream().map(Student::getAge).collect(Collectors.toSet()); // [20, 10]  //Collectors.toMap() 转成map,注:key不能相同，否则报错 Map&amp;lt;String, Integer&amp;gt; studentMap = list3.stream().collect(Collectors.toMap(Student::getName, Student::getAge)); // {cc=10, bb=20, aa=10}  //Collectors.joining(&amp;#34;,&amp;#34;, &amp;#34;(&amp;#34;, &amp;#34;)&amp;#34;) 字符串分隔符连接 String joinName = list3.stream().map(Student::getName).collect(Collectors.joining(&amp;#34;,&amp;#34;, &amp;#34;(&amp;#34;, &amp;#34;)&amp;#34;)); // (aa,bb,cc)  //Collectors 聚合操作 //学生总数 Long count2 = list3.stream().collect(Collectors.counting()); // 3 //最大年龄 (最小的minBy同理) Integer maxAge = list3.stream().map(Student::getAge).collect(Collectors.maxBy(Integer::compare)).get(); // 20 //所有人的年龄 Integer sumAge = list3.stream().collect(Collectors.summingInt(Student::getAge)); // 40 //平均年龄 Double averageAge = list3.stream().collect(Collectors.averagingDouble(Student::getAge)); // 13.333333333333334 // 带上以上所有方法 DoubleSummaryStatistics statistics = list3.stream().collect(Collectors.summarizingDouble(Student::getAge)); System.out.println(&amp;#34;count:&amp;#34; &#43; statistics.getCount() &#43; &amp;#34;,max:&amp;#34; &#43; statistics.getMax() &#43; &amp;#34;,sum:&amp;#34; &#43; statistics.getSum() &#43; &amp;#34;,average:&amp;#34; &#43; statistics.getAverage()); //分组，按组生成map Map&amp;lt;Integer, List&amp;lt;Student&amp;gt;&amp;gt; ageMap = list3.stream().collect(Collectors.groupingBy(Student::getAge)); //分区 //分成两部分，一部分大于10岁，一部分小于等于10岁 Map&amp;lt;Boolean, List&amp;lt;Student&amp;gt;&amp;gt; partMap = list3.stream().collect(Collectors.partitioningBy(v -&amp;gt; v.getAge() &amp;gt; 10)); //reduce Integer allAge = list3.stream().map(Student::getAge).collect(Collectors.reducing(Integer::sum)).get(); //40/** * 3. reduce操作, reduce 是一种归约操作，将流归约成一个值的操作叫做归约操作，用函数式编程语言的术语来说，这种称为折叠（fold） * Optional&amp;lt;T&amp;gt; reduce(BinaryOperator&amp;lt;T&amp;gt; accumulator)：第一次执行时，accumulator函数的第一个参数为流中的第一个元素，第二个参数为流中元素的第二个元素；第二次执行时，第一个参数为第一次函数执行的结果，第二个参数为流中的第三个元素；依次类推。 * T reduce(T identity, BinaryOperator&amp;lt;T&amp;gt; accumulator)：流程跟上面一样，只是第一次执行时，accumulator函数的第一个参数为identity，而第二个参数为流中的第一个元素。 * &amp;lt;U&amp;gt; U reduce(U identity,BiFunction&amp;lt;U, ? super T, U&amp;gt; accumulator,BinaryOperator&amp;lt;U&amp;gt; combiner)：在串行流(stream)中，该方法跟第二个方法一样，即第三个参数combiner不会起作用。 * 在并行流(parallelStream)中,我们知道流被fork join出多个线程进行执行，此时每个线程的执行流程就跟第二个方法reduce(identity,accumulator)一样，而第三个参数combiner函数，则是将每个线程的执行结果当成一个新的流，然后使用第一个方法reduce(accumulator)流程进行reduce。 * */ List&amp;lt;Integer&amp;gt; list4 = Arrays.asList(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24); //3.1 Optional&amp;lt;T&amp;gt; reduce(BinaryOperator&amp;lt;T&amp;gt; accumulator)方法，效果是x1&#43;...&#43;xn Integer v = list4.stream().reduce((x1, x2) -&amp;gt; x1 &#43; x2).get(); System.out.println(v); // 300  //3.2 T reduce(T identity, BinaryOperator&amp;lt;T&amp;gt; accumulator)方法，效果是10&#43;x1&#43;...&#43;xn Integer v1 = list4.stream().reduce(10, (x1, x2) -&amp;gt; x1 &#43; x2); System.out.println(v1); //310  //3.3 &amp;lt;U&amp;gt; U reduce(U identity,BiFunction&amp;lt;U, ? super T, U&amp;gt; accumulator,BinaryOperator&amp;lt;U&amp;gt; combiner)方法 Integer v2 = list4.stream().reduce(0, (x1, x2) -&amp;gt; { System.out.println(&amp;#34;stream accumulator: x1:&amp;#34; &#43; x1 &#43; &amp;#34; x2:&amp;#34; &#43; x2); return x1 - x2; }, (x1, x2) -&amp;gt; { System.out.println(&amp;#34;stream combiner: x1:&amp;#34; &#43; x1 &#43; &amp;#34; x2:&amp;#34; &#43; x2); return x1 * x2; }); System.out.println(v2); // -300  Integer v3 = list4.parallelStream().reduce(0, (x1, x2) -&amp;gt; { System.out.println(&amp;#34;parallelStream accumulator: x1:&amp;#34; &#43; x1 &#43; &amp;#34; x2:&amp;#34; &#43; x2); return x1 - x2; }, (x1, x2) -&amp;gt; { System.out.println(&amp;#34;parallelStream combiner: x1:&amp;#34; &#43; x1 &#43; &amp;#34; x2:&amp;#34; &#43; x2); return x1 * x2; }); System.out.println(v3); //197474048 Optional类 很多的stream的终端操作，都返回了一个Optional对象。Optional 类主要解决的问题是臭名昭著的空指针异常（NullPointerException）。
//1. Optional创建方法 //这三个创建方法建议使用Optional.ofNullable。比如从数据库/API获取到的结果，不知道其是否为null,此时直接使用此方法即可 List&amp;lt;String&amp;gt; list = Arrays.asList(&amp;#34;aaa&amp;#34;); Optional optional1 = Optional.of(list); //此方法如果list为null，会抛异常，不建议使用 Optional optional2 = Optional.ofNullable(list); //此方法如果list为null，不会抛异常，建议使用  //2. 获取元素方法 optional1.get(); //此方法如果optional1包装的对象为null，会抛异常，不建议使用 optional1.orElse(Arrays.asList(&amp;#34;bbb&amp;#34;)); //当optional1包装的对象为null时，使用给定的默认值返回 optional1.orElseGet(() -&amp;gt; Arrays.asList(&amp;#34;bbb&amp;#34;)); //当optional1包装的对象为null时，使用默认的Supplier函数式接口返回值  //3. 判断是否存在的方法 optional1.isPresent(); //此方法判断optional1的包装对象是否为null，返回boolean结果 optional1.ifPresent(System.out::println); //此方法是在optional1的包装对象存在的情况下，遍历打印出元素  //例子  Student student1 = new Student(); Student student2 = new Student(&amp;#34;apple&amp;#34;, 10); Student student3 = null; Optional&amp;lt;Student&amp;gt; optional11 = Optional.ofNullable(student1); optional11.ifPresent(System.out::println); Optional&amp;lt;Student&amp;gt; optional22 = Optional.ofNullable(student2); String result = optional22.map(Student::getName).filter(name -&amp;gt; name.equals(&amp;#34;car&amp;#34;)).orElse(&amp;#34;default name&amp;#34;); System.out.println(result); Optional&amp;lt;Student&amp;gt; optional33 = Optional.ofNullable(student3); optional33.orElseThrow(() -&amp;gt; new RuntimeException(&amp;#34;exception&amp;#34;)); Numeric Stream IntStream,LongStream,DoubleStream分别表示原始 int 流、 原始 long 流 和 原始 double 流，提供了大量的静态方法，对数字类型的集合操作，建议转成流进行处理。
常用方法有
   方法 说明     rangeClosed(a,b) 返回子序列 [a,b]，左闭右闭（包括 b 元素，增长步值为 1）   range(a,b) 返回子序列 [a,b)，左闭右开（不包括 b）   sum 计算总和   sorted 排序   toArray 流转为数组   max 最大值   min 最小值   average 平均值   of 传入数组，转成流   </content>
    </entry>
    
     <entry>
        <title>【java8】(二)高大上的函数式接口怎么用</title>
        <url>https://jasonqian10.github.io/post/java/java8-function-interface/</url>
        <categories>
          <category>java</category>
        </categories>
        <tags>
          <tag>java8</tag><tag>函数式接口</tag>
        </tags>
        <content type="html"> 函数式接口 函数式接口(Functional Interface)是只有一个抽象方法的接口。这样一个函数接口其实就是一个函数，将函数式编程转换成对象操作。
1)函数式接口中的抽象函数就是为了支持 lambda表达式；
2)函数式接口可以被隐式转换为lambda表达式；
3)为确保函数式接口符合语法，可以添加@FunctionalInterface注解；
注意：加不加@FunctionalInterface对于接口是不是函数式接口没有影响，该注解知识提醒编译器去检查该接口是否仅包含一个抽象方法。
例外：
函数式接口里是可以包含默认方法default method（函数式接口里是可以包含默认方法）；
函数式接口里是可以包含静态方法（因为静态方法不能是抽象方法，是一个已经实现了的方法，无法被覆写）；
函数式接口里是可以包含Object里的public方法，如equals()。
下面介绍几种常用的函数式接口。
Predicate 主要用于filter条件判断或者断言，接受一个泛型的对象T,调用boolean testT t)方法，返回boolean结果。
常用场景：list.stream().filter(Predicate p)
Stream&amp;lt;T&amp;gt; filter(Predicate&amp;lt;? super T&amp;gt; predicate); Stream的filter接口的入参就是Predicate函数式接口的lambda表达式。
例子：
//通过Color过滤，入参是 Predicate&amp;lt;T&amp;gt; predicate public static List&amp;lt;Apple&amp;gt; filterByColor(List&amp;lt;Apple&amp;gt; list, Predicate&amp;lt;Apple&amp;gt; predicate) { List&amp;lt;Apple&amp;gt; result = new ArrayList&amp;lt;&amp;gt;(); for (Apple apple : list) { if (predicate.test(apple)) { result.add(apple); } } return result; } //初始化list List&amp;lt;Apple&amp;gt; list = Arrays.asList(new Apple(&amp;#34;green&amp;#34;, 120), new Apple(&amp;#34;red&amp;#34;, 130)); /* Predicate&amp;lt;? super T&amp;gt; predicate = apple -&amp;gt; apple.getColor().equals(&amp;#34;green&amp;#34;)) 这里的Predicate lambda表达式的泛型T表示Apple类 */ System.out.println(filterByColor(list, apple -&amp;gt; apple.getColor().equals(&amp;#34;green&amp;#34;))); 我们可以通过stream的filter接口来实现同样的功能
/* Predicate&amp;lt;? super T&amp;gt; predicate = l -&amp;gt; l.getColor().equals(&amp;#34;green&amp;#34;) 这里的Predicate lambda表达式的泛型T表示String类 */ System.out.println(list.stream().filter(l -&amp;gt; l.getColor().equals(&amp;#34;green&amp;#34;)).collect(Collectors.toList())); Predicate接口还有一些default方法：
default Predicate&amp;lt;T&amp;gt; and(Predicate&amp;lt;? super T&amp;gt; other) default Predicate&amp;lt;T&amp;gt; or(Predicate&amp;lt;? super T&amp;gt; other) 通过predicate.or() predicate.and()实现filter条件组合。
Consumer consumer主要是消费数据，接受一个泛型的对象T，调用void accept(T t)方法，无返回结果。 其实就是执行一段自定义的逻辑。
常用场景：list.steam().forEach() 等执行业务逻辑、无返回结果的函数式接口。
void forEach(Consumer&amp;lt;? super T&amp;gt; action); 例子：
//定义consumer方法 public static void consumer(List&amp;lt;Apple&amp;gt; list, Consumer&amp;lt;Apple&amp;gt; consumer) { // return list.stream().filter(l -&amp;gt; predicate.test(l.getColor())).collect(Collectors.toList());  for (Apple apple : list) { consumer.accept(apple); } } /* Consumer&amp;lt;Apple&amp;gt; consumer = apple -&amp;gt; System.out.println(apple.getColor()); */ consumer(list, apple -&amp;gt; System.out.println(apple.getColor())); 可以通过forEach接口实现同样的功能
list.stream().forEach(l -&amp;gt; System.out.println(l.getColor())); Consumer接口还有如下default方法
default Consumer&amp;lt;T&amp;gt; andThen(Consumer&amp;lt;? super T&amp;gt; after) 消费数据的时候，首先做一个操作，然后再做一个操作，实现组合。
Supplier 接口仅包含一个无参的方法： T get() 。用来获取一个泛型参数指定类型的对象数据。由于这是一个函数式接口，这也就意味着对应的Lambda表达式需要“对外提供”一个符合泛型类型的对象数据。
例子：
Supplier&amp;lt;Apple&amp;gt; supplier = () -&amp;gt; new Apple(&amp;#34;&amp;#34;, 0); Apple apple = supplier.get(); Function java.util.function.Function 接口用来根据一个类型的数据得到另一个类型的数据，前者称为前置条件，后者称为后置条件将T 转为 R，是一个类型转换接口
常用场景：list.stream().map()
&amp;lt;R&amp;gt; Stream&amp;lt;R&amp;gt; map(Function&amp;lt;? super T, ? extends R&amp;gt; mapper); Stream的map接口接收的就是Function函数式接口对象，将集合中的元素转换成另一个类型。
例子：
Function&amp;lt;Apple, String&amp;gt; function = apple -&amp;gt; apple.getColor(); function.apply(new Apple(&amp;#34;red&amp;#34;,100));</content>
    </entry>
    
     <entry>
        <title>【java8】(一)lambda表达式你用对了吗？</title>
        <url>https://jasonqian10.github.io/post/java/java8-lambda/</url>
        <categories>
          <category>java</category>
        </categories>
        <tags>
          <tag>java8</tag><tag>lambda表达式</tag>
        </tags>
        <content type="html"> 前言 Java8引入了lambda表达式。随着java8应用的越来越广，在我们开发过程中已经大量的在使用lambda表达式了，但是使用了并不说明你真正懂了lambda，离开idea的自动联想你能直接写出lambda表达式吗？
lambda表达式语法 概念 可以把Lambda表达式理解为简洁的表示可传递的匿名函数(函数式接口)的一种方式：它没有名称，但它有参数列表，函数主体，返回类型，可能还有一个可以抛出的异常列表。
 lambda表达式依附于函数式接口，可以理解为是函数式接口的匿名实现。
函数式接口后面会详细介绍。
 语法格式 java8中，引入了一个新的操作符“-&amp;gt;”：
左侧：lambda表达式参数列表。这个参数列表是接口的入参。
右侧：lambda表达式所需执行的功能，即lambda函数体。这个是接口的具体实现函数。
无参数，无返回值用法 //应用lambda表达式之前 Runnable r1 = new Runnable() { @Override public void run() { System.out.println(&amp;#34;无参数，无返回值&amp;#34;); } }; r1.run(); //应用lambda表达式之前 Runnable r2 = () -&amp;gt; System.out.println(&amp;#34;无参数，无返回值&amp;#34;); r2.run(); Runnable接口(Java8改造成一种函数式接口了) 就是场景的例子。 这里-&amp;gt;左侧参数列表即使接口函数public void run()的入参；右侧是run()接口的实现体。
无参数，有返回值 //定义获取状态的接口 @FunctionalInterface public interface Status { public String getStatus(); } //应用lambda表达式之前 Status status1 = new Status() { @Override public String getStatus() { return &amp;#34;stop&amp;#34;; } }; System.out.println(status1.getStatus()); //应用lambda表达式之后 Status status2 = () -&amp;gt; &amp;#34;stop&amp;#34;; System.out.println(status2.getStatus()); 与上面一类场景相比，这里右侧的函数体是有返回值的。可以通过函数体最终结果是void还是Object来判断是否有返回值。
注意有返回值时右侧函数体如果只有一行，可以省略return和{},即也可以这样写
//lambda写法1 Status status2 = () -&amp;gt; &amp;#34;stop&amp;#34;; //lambda写法2 Status status = () -&amp;gt; { return &amp;#34;stop&amp;#34;; }; //建议第一种 有1个参数，无返回值 //定义打印信息的接口 @FunctionalInterface public interface Printer { public void print(String str); } //应用lambda表达式之前 Printer printer1 = new Printer() { @Override public void print(String str) { System.out.println(str); } }; printer1.print(&amp;#34;123&amp;#34;); //应用lambda表达式之后 Printer printer2 = (str -&amp;gt; System.out.println(str)); printer2.print(&amp;#34;123&amp;#34;); 这里注意有1个参数时，左侧的参数括号可以省略。下面是几种扩展写法
Printer printer2 = (str -&amp;gt; System.out.println(str)); Printer printer2 = ((str) -&amp;gt; System.out.println(str)); Printer printer2 = ((String str)-&amp;gt; System.out.println(str)); 有1个参数，有返回值 //定义int到String转换接口 @FunctionalInterface public interface Exchanger { public String exchange(int n); } Exchanger exchanger1 = new Exchanger() { @Override public String exchange(int n) { return String.valueOf(n); } }; System.out.println(exchanger1.exchange(10)); Exchanger exchanger2 = (n -&amp;gt; String.valueOf(n)); System.out.println(exchanger2.exchange(20)); 有多个参数，无返回值 //定义加法接口 @FunctionalInterface public interface Operation { public int plus(int a, int b); } Printer2 printer21 = new Printer2() { @Override public void print(int a, int b) { System.out.println(a&#43;b); } }; printer21.print(5,3); Printer2 printer22 = (a,b) -&amp;gt; System.out.println(a&#43;b); printer22.print(20,30); 这里要注意左侧参数列表有多个参数时，括号一定要保留。下面Lambda多种扩展写法。
Printer2 printer22 = (a,b) -&amp;gt; System.out.println(a&#43;b); Operation operation2 = (int a,int b) -&amp;gt; { System.out.println(a&#43;b) }; 有多个参数，有返回值 //这里我们直接使用现有的Comparator接口 //应用lambda表达式之前 Comparator comparator1 = new Comparator() { @Override public int compare(Object o1, Object o2) { return o1.hashCode() - o2.hashCode(); } }; System.out.println(comparator1.compare(new Integer(5), new Integer(10))); //应用lambda表达式之后 Comparator comparator2 = ((o1, o2) -&amp;gt; o1.hashCode() - o2.hashCode()); System.out.println(comparator2.compare(new Integer(3), new Integer(10))); 当右侧函数体有多行时，一定需要{}
总结  左侧参数列表  无参数和有多个参数时一定需要括号()
有一个参数时，可以省略括号()
当使用括号()时，参数的类型可以省略不写
 右侧函数体  单行代码：可以省略花括号{}和return(无论是否有返回值)
多行代码：一定要写花括号{}和return(如果有返回值)
</content>
    </entry>
    
     <entry>
        <title>Intellij idea搭建solr源码debug调试环境</title>
        <url>https://jasonqian10.github.io/post/solr-debug-idea-build/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>debug</tag><tag>Intellij idea</tag>
        </tags>
        <content type="html"> solr源码debug环境对于研究solr和Lucene源码很有必要，本文总结本人搭建的详细步骤。
clone代码 git clone https://github.com/apache/lucene-solr.git
下载后阅读根目录下README.md。
下载ant依赖 如果本地没有安装过ant的请自行安装。
在项目根目录下执行命令
ant ivy-bootstrap
自动下载ant依赖包。第一次执行需要较长时间，显示BUILD SUCCESSFUL则成功。
 这一步只需要执行一次，后面不需重复执行。
 ant项目转换成maven项目 项目根目录下执行命令
ant idea
 注意：执行完此步骤才可以用intellj idea打开项目。未转换先打开了会有问题。
 编译项目 进入项目根目录 solr/ 目录，执行命令
ant server
idea运行solr server 在idea上运行solr server是实际启动的是embedSolrServer。
 先准备一个solr_home，放在solr/server/ 目录下
 在项目 solr/solrj/src/test/org/apache/solr/client/solrj/StartSolrJetty.java 中增加上面准备的solr_hom的路径；设置webapp war路径
   solr/solrj/src/resources 目录下添加log4j2配置文件（solr/solrj/src/resources/log4j2.xml）
 run StartSolrJetty.java，然后访问 localhost:8983/solr
   对源码打断点，debug StartSolrJetty.java 即可进行源码调试  导入自定义jar 如果有基于源码自定义的插件jar，也可以把插件代码导入到源码项目中进行debug。
</content>
    </entry>
    
     <entry>
        <title>搜索引擎中相似度算法TF-IDF和BM25</title>
        <url>https://jasonqian10.github.io/post/nlp-tf-idf-bm25/</url>
        <categories>
          <category>Lucene</category>
        </categories>
        <tags>
          <tag>TF-IDF</tag><tag>BM25</tag>
        </tags>
        <content type="html">  前言 当我们使用搜索引擎时，它总是会把相关性高的内容显示在前面，相关性低的内容显示在后面。那么，搜索引擎是如何计算关键字和内容的相关性呢？这里介绍2种重要的相似度算法：TF-IDF和BM25。
TF-IDF是Lucene上一代（6.0以前）相似度算法，BM25是Lucene新一代（6.0以后）正使用的相似度算法。
先举个例子。假如，我们想找和“Lucene”相关的文章。可以想一下，那些内容里只出现过一次“Lucene”的文章，有可能是在讲某种技术，顺便提到了Lucene这个工具。而那些出现了两三次“Lucene”的文章，很可能是专门讨论Lucene的。通过直觉，我们可以得出判断：关键字出现的次数越多，文档与关键字的匹配度越高。
TF Term Frequency，缩写为TF。通常叫做“词频”，表示文档中关键字出现的次数。
 通常TF越大，相关性越高。
 但是，你可能会发现一个问题。例如一篇小短文里出现了一次“Lucene”，而一部好几百页的书里提到两次“Lucene”，此时我们就不能说后者相关度更高了。为了消除文档本身大小的影响，在计算TF时引入文档长度这个参数，做文档长度标准化
TF socre ＝ 某个词在文档中出现的次数 ／ 文档的长度 举例：某文档D，长度为200，其中“Lucene”出现了2次，“的”出现了20次，“原理”出现了3次，那么
TF(Lucene|D) = 2/200 = 0.01 TF(的|D) = 20/200 = 0.1 TF(原理|D) = 3/200 = 0.015 其中我们发现“的”这个词的比重非常高，搜索引擎中一般把这种“的、地、得”这些虚词去掉，不计算其分数。
IDF Inverse Dcument Frequency, 缩写为IDF。通常叫做“逆文档频率”，其定义为
IDF = log(语料库的文档总数 / 包含该词的文档数 &#43; 1) 可见包含该词的文档数越小，分母就越小，IDF越大；该词越常见，分母就越大，IDF越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。
举例：世界上文档总数位100亿，&amp;rdquo;Lucene&amp;rdquo;在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
IDF(Lucene) = log(100亿/1万&#43;1) = 19.93 IDF(原理) ＝ log(100亿/2亿&#43;1) ＝ 5.64 “Lucene”重要性相当于“原理”的3.5倍，可见“Lucene”更能表征这篇文档。
TF-IDF TF-IDF算法的相似度公式就是TF和IDF的加权求和
simlarity = TF1*IDF1 &#43; TF2*IDF2 &#43; ... &#43; TFn*IDFn 上面的例子最终相似度得分为
simlarity(Lucene的原理|D) = 0.01*19.93 &#43; 0 &#43; 5.64*0.015 ＝ 0.2839 其中，“Lucene”占了总分70%的权重，“原理”仅占总分30%的权重。
Lucene中的TF-IDF Lucene对TF-IDF算法做了适当调整，它的相似度公式为
simlarity = log(numDocs / (docFreq &#43; 1)) * sqrt(tf) * (1/sqrt(length)) 各参数含义
 numDocs： 索引的文档总数量 docFreq： 包含关键字的文档数量 tf：关键字在一篇文档中出现的次数。 length：文档的长度  上面的公式在Lucene打分计算时会被拆分成三个部分：
IDF Score = log(numDocs / (docFreq &#43; 1)) TF Score = sqrt(tf) fieldNorms = 1/sqrt(length)  log()，sqrt()主要是为了降低分值大小，缩写文档之间分数差距
fieldNorms是对文本长度的归一化(Normalization)，为了消除文档长度对分数的影响
 所以，上面公式也可以表示成
simlarity = IDF score * TF score * fieldNorms Lucene在score打分时会遍历匹配到的每篇文档计算每篇文档的score，每篇文档分别计算这三部分分值。最后默认根据score sort。
BM25,下一代的TF-IDF 新版的Lucene不再把TF-IDF作为默认的相关性算法，而是采用了BM25(BM是Best Matching的意思)。BM25是基于TF-IDF并做了改进的算法。先来看下改进之后的BM25算法
simlarity = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) * ((k1 &#43; 1) * tf) / (K &#43; tf) 上面的公式在Lucene打分计算时会被拆分成两部分：
IDF Score = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) TF Score = ((k1 &#43; 1) * tf) / (K &#43; tf)  这里k1和K下面展开讨论
  #### TF项比较  两者公式如下
传统 TF Score = sqrt(tf) BM25的 TF Score = ((k1 &#43; 1) * tf) / (K &#43; tf) 传统的TF值理论上是可以无限大的。而BM25与之不同，它在TF计算方法中增加了一个常量k1和K，用来限制TF值的增长极限，BM25的 TF Score的范围会控制在[0, k1&#43;1]。其中K
K = k1(1 - b &#43; b*dl/avgdl) 取值k1，b是调节因子，Lucene中默认是k1=2，b=0.75，用户可以自定义。dl为文档的长度，avgdl为所有文档的平均长度。
两者的分布曲线来看，随着tf增大，BM25的TF会接近k1&#43;1，传统的TF会无限变大。
TF中还引入了文档的长度dl，所有文档的平均长度avgdl，这里假设L = dl/avgdl，下面是不同L的条件下，词频对TFScore影响的走势图
从图上可以看到，文档越短，它逼近上限的速度越快，反之则越慢。这是可以理解的，对于只有几个词的内容，比如文章“标题”，只需要匹配很少的几个词，就可以确定相关性。而对于大篇幅的内容，比如一本书的内容，需要匹配很多词才能知道它的重点是讲什么。所以在专利检索中相关性通常的顺序 TTL &amp;gt; ABST &amp;gt; CLMS &amp;gt; DESC。
关于参数b的作用：公式中L前面有个常系数b，如果把b设置为0，则L完全失去对评分的影响力。b的值越大，L对总评分的影响力越大。
 IDF项比较  两者公式如下
传统的 IDF Score = log(numDocs / (docFreq &#43; 1)) BM25的 IDF Score = log(1 &#43; (numDocs - docFreq &#43; 0.5) / (docFreq &#43; 0.5)) 从分布曲线来看两者走势基本一致。
TF-IDF vs BM25 传统的TF-IDF是自然语言搜索的一个基础理论，它符合信息论中的熵的计算原理，你观察IDF公式会发现，它与熵的公式是类似的。实际上IDF就是一个特定条件下关键词概率分布的交叉熵。
BM25在传统TF-IDF的基础上增加了几个可调节的参数，使得它在应用上更佳灵活和强大，具有较高的实用性。
</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】hadoop调优经验</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-3/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag>
        </tags>
        <content type="html"> 【Hadoop索引】本篇系列第三篇。主要介绍Hadoop调试过程中的一些问题以及解决方法的总结。
主要有以下几类问题
 资源分配问题
 性能调优问题
 Yarn框架运行机制问题
 Solr索引问题
  资源分配问题 MapReduce集群任务分配机制 举例：一套机型为c4.4x的16个node节点的emr集群为例
 c4.4x机器配置16核，30G内存
 MapReduce集群最多可运行多少个container资源？ 默认
以机器的vcode数量作为container数量，总 16* 16=256 container，也就是集群做多可以同时跑256个task。每个container的含有资源为 1core，30*0.75&amp;frasl;16=1.4G。
 emr默认会用机器75%的内存来分配
 实际并发container数量 阶段 | 实际设定的内存 | 单node并发的container数量 | 集群总并发container数量 -|-|-|- Map | 3G | 30*0.75&amp;frasl;3=7 |7*16=112 Reduce | 16G | 30*0.75&amp;frasl;16=1 |1*16=16
MapReduce job会被拆分成多少个map/reduce task？ 阶段 | task数量 -|-|-|- Map | input size/ hdfs block size(split size) Reduce | mapreduce.job.reduces设定 &amp;gt; 默认情况下hdfs block size(split size)为64M，可以根据业务需要调整大小来控制Map task数量。
内存溢出问题 问题现象：提示“Container is running beyond physical memory”、“running beyond virtual memory limits”
原因：container设定的物理内存、虚拟内存不足导致失败。
解决：
1）一般内存溢出大部分是用户自己代码问题，可以优化代码是内存使用降低
2）如果必须需要使用大量内存，可以通过配置提高内存 参数 |说明 -|- mapreduce.map.memory.mb |Map container内存 mapreduce.map.java.opts |Map jvm内存，一般为Map container内存的80% mapreduce.reduce.memory.mb |Reduce container内存 mapreduce.reduce.java.opts |Reduce jvm内存，一般为Reduce container内存的80% yarn.nodemanager.vmem-pmem-ratio |虚拟内存和物理内存比值，默认5。如果虚拟内存需要太大，可以通过提高这个参数 yarn.nodemanager.vmem-check-enabled |虚拟内存检查，默认true。false可以关闭虚拟内存阈值check，一般不建议修改。 yarn.nodemanager.pmem-check-enabled |物理内存检查，默认true。false可以关闭物理内存阈值check，一般不建议修改。
性能调优问题 shuffle阶段性能调优配置 参数 |说明 -|- mapreduce.job.reduce.slowstart.completedmaps | 这个参数表示map完成多少进度后开始启动reduce container。默认是0.5。建议设置0.95，因为reduce container过早启动会抢占map的资源，导致并发map的数量减少，reduce正式开始跑需要等待map 100%完成，启动早无太大实质效果，只是决定早点开始拷贝map output。 mapreduce.reduce.shuffle.merge.percent | reduce 复制的数据先写到 reduce 任务的 JVM 内存缓存区，这个参数表示内存缓存区占reduce jvm比重阀值，超过该比例将进行合并和溢写磁盘。默认是0.66，建议设小以免shuffle时reduce jvm内存爆。 mapreduce.reduce.shuffle.memory.limit.percent | 单个shuffle线程能使用的内存限额，默认是0.25，并发线程数多于4之后容易内存爆掉，建议设置小于0.15。 mapreduce.reduce.shuffle.parallelcopies | reduce并行copy maptask输出文件的线程数量，默认是5，可以配置（根据集群节点间的带宽） mapreduce.reduce.shuffle.read.timeout | shuffle copy的超时时间，默认是180000。如果集群节点间带宽较低，可以增加超时时间，否则可能会copy timeout。 |
|
Yarn框架运行机制问题 Task运行超时被kill掉 问题：Task由于代码堵塞或者业务逻辑执行时间过长，长时间不结束，TaskTracker把这个Task kill掉。
原因：每个Task会定期向TaskTracker汇报进度，如果进度不变则不汇报，这样一旦达到超时限制，TaskTracker会杀掉该任务，并将任务状态KILLED汇报给YARN,从而重新调度该任务。
解决：
1）Task主动上报状态，使得TaskTracker知道Task正在运行。可以调用MapReduce的统计相关的sdk，如TaskAttemptContext.progress()、TaskAttemptContext.getCounter()等sdk。
2）mapreduce.task.timeout，mr程序的task执行情况汇报过期时间，默认600000(10分钟)，可以设大。
shuffle copy time out 问题：shuffle阶段reducer copy各个节点上map的输出文件，报“Failed to connect to xxx with 1 map outputs java.net.SocketTimeoutException: Read timed out”
原因：由于机器ebs带宽瓶颈，copy超时。
解决：这个原因其实是机器性能问题，无法直接进行提升。可以通过以下方式进行规避。
1）mapreduce.reduce.shuffle.parallelcopies
参数是copy并发线程数量，建议设小
2）mapreduce.reduce.shuffle.read.timeout
参数表示shuffle copy的超时时间，默认是180000，建议设大
3）mapreduce.reduce.maxattempts，mapreduce.map.maxattempts
参数表示reduce、map task失败重试次数，这种网络拷贝问题是并发时偶现问题，增加重试次数
Solr索引问题 reduce阶段做solr index和optimize爆内存 问题：solr index和optimize时reduce container内存爆掉
原因：solr index默认使用MMapDirectoryFactory，这个目录实现类适用虚拟内存和内核中一个叫 mmap 的特性来访问存在磁盘上的文件。它允许 Lucene 直接访问 I/O 缓存，当不需要准实时搜索时，这个目录实现是一个非常不错的选择。MMap由于使用了memory mapping，会使用到跟索引总大小近似的virtual memory。
解决：替换成NIOFSDirectoryFactory，在solr index和optimize过程中进行内存使用基本平稳。
</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】Mapreduce进行solr索引实战</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-2/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>solr</tag>
        </tags>
        <content type="html"> 【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第二篇，介绍如何应用Hadoop到solr index。
为什么使用Hadoop来做solr索引？ 这与Hadoop特点有关，hadoop适用于处理大数据并行计算，提高solr索引速度。Hadoop的MapReduce计算框架分map阶段和reduce阶段。map阶段转换solr document，reduce阶段进行solr index操作。MapReduce流程契合solr索引的流程。
如何用MapReduce做solr索引？ 网上现在使用MapReduce做solr索引的方式实战比较少，常见的尝试有如下两种
 Cloudera MapReduceIndexerTool  cloudera提供了基于morphline的索引工具SDK，solr也做了集成，在solr/contrib/map-reduce包。入口类为 MapReduceIndexerTool，详细使用可以参考
https://docs.cloudera.com/documentation/enterprise/latest/topics/search_mapreduceindexertool.html
 running solr on HDFS  这个是solr支持的索引到HDFS上，其实与MapReduce没有啥关系，只是存储文件系统换成了HDFS，细节见
https://lucene.apache.org/solr/guide/6_6/running-solr-on-hdfs.html
作者起先尝试过Cloudera MapReduceIndexerTool的方式，发现有些定制化的功能无法满足，而且solr在6.6版本之后移除了map-reduce模块（https://issues.apache.org/jira/browse/SOLR-9221 ），原因是solr与Hadoop版本兼容问题，所以最后放弃使用了。但是solr/contrib/map-reduce中很多源码是值得参考的，我们在开发过程了也借鉴了很多。
运行流程  备注: 我们使用aws emr做Hadoop集群，所以会使用一些 aws的服务
 流程其实很清晰，主要有以下几部分
 Input  做索引的输入文件，存在在aws s3或者hdfs。
job client提交job任务，初始化MapReduce job object。
 Map  map阶段完成solrDoucment的转换，其map的输出为，其中shardNum表示solrDocument所在solr集群中哪个shard。因为我们使用embedSolr作为solrServer进行index，并未使用https的方式进行index，所以分shard的逻辑需要在map中进行计算。算出shardNum，作为map输出的key。这样在后续shuffle，sort过程中MapReduce框架会把key相同的所有solrDoucment拷贝到同一reduce task的节点上。
 Reduce  包含前置流程shuffle和sort，目的是把Hadoop集群各个节点上map输出的solrDoucment按key的不同拷贝并排序到对应的reduce task节点上。
Reduce task就纯粹完成solr index，reduce container的jvm中启动EmbeddedSolr作为solr server。
 output  solr索引文件支持输出到hdfs或者disk（只需在solrconfig.xml中配置即可），两种方式作者均尝试过，index速度方面，存储到disk更快一些。如果对index速度要求不是很高，建议使用hdfs，因为使用了Hadoop，最好使用它的分布式存储系统。
代码实现 job client Job client main()方法
//初始化Job object Job.getInstance(conf, &amp;#34;solr index&amp;#34;) Configuration conf = job.getConfiguration(); //设置MapReduce入口类（job client类） job.setJarByClass(Runner.class); //设置MapReduce输入文件 FileInputFormat.addInputPath(job, new Path(INPUT_PATH))); //设置blocksize大小，INPUT_PATH会按照splitSize大小进行分割成多个 FileInputFormat.setMinInputSplitSize(job, splitSize); FileInputFormat.setMaxInputSplitSize(job, splitSize); //SolrOutputFormat.java继承FileOutputFormat.java。solr/contrib/map-reduce包中存在这个类 job.setOutputFormatClass(SolrOutputFormat.class); //map output的key类型 job.setOutputKeyClass(IntWritable.class); //设置map多线程 MultithreadedMapper.setMapperClass(job, SolrMapper.class); MultithreadedMapper.setNumberOfThreads(job, MAP_THREAD_NUM); job.setMapperClass(MultithreadedMapper.class); //map output的value类型 job.setOutputValueClass(SolrInputDocumentWritable.class); //优先使用jar包classpath的类，其次才会使用Hadoop环境中lib的classpath类。为了解决不同版本类冲突 conf.setBoolean(&amp;#34;mapreduce.job.user.classpath.first&amp;#34;, true); //设置reduce task数量 job.setNumReduceTasks(REDUCE_TASK_NUM)); 如上源码是初始化job client核心代码。如果jar包运行过程中涉及一些配置文件，需要传至各个node节点，则可以参考 SolrOutputFormat#setupResourceCache这个方法。
map SolrMapper.java
public class SolrMapper extends Mapper&amp;lt;LongWritable, Text, IntWritable, SolrInputDocumentWritable&amp;gt; { @Override protected void setup(Context context) throws IOException { //初始化  } @Override protected void cleanup(Context context) { } @Override public void map(LongWritable key, Text value, Context context) throws InterruptedException, IOException { SolrInputDocument doc = xxx; //solr document转换逻辑  int shardOrd = xxx; //solr分shard逻辑，确定doc的分片  //统计  context.getCounter(Counters.MAPPER).increment(1); //map outout  context.write(new IntWritable(shardOrd), new SolrInputDocumentWritable(doc)); } } 其中SolrInputDocumentWritable.java来自solr/contrib/map-reduce包。
reduce SolrReducer.java
public class SolrReducer&amp;lt;K, V&amp;gt; extends RecordWriter&amp;lt;K, V&amp;gt; { private SolrClient solrClient; public SolrReducer(TaskAttemptContext context) { //初始化  this.solrClient = SolrUtil.createEmbeddedSolr(); } /** * Write a record. This method accumulates records in to a batch, and when * items are present flushes it to the indexer. The writes * can take a substantial amount of time, depending on. If * there is heavy disk contention the writes may take more than the 600 second * default timeout. */ @Override public void write(K key, V value) { SolrInputDocumentWritable sidw = (SolrInputDocumentWritable) value; SolrInputDocument sid = sidw.getSolrInputDocument(); solrClient.add(sid); } @Override public void close(TaskAttemptContext context) { if (solrClient != null) { solrClient.close(); } }</content>
    </entry>
    
     <entry>
        <title>【Hadoop索引】我理解的hadoop</title>
        <url>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-1/</url>
        <categories>
          <category>Hadoop</category>
        </categories>
        <tags>
          <tag>hadoop</tag><tag>mapreduce</tag><tag>solr</tag>
        </tags>
        <content type="html"> 【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第一篇，介绍我学习使用Hadoop后对其的理解。
Hadoop概念 官方概念 The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using simple programming models.
总结：处理大数据、分布式集群、可编程计算框架
Hadoop 的核心组件 Hadoop由1.0和2.0两个架构版本，2.0最大的变化就是引入YARN这个资源调度框架，更加精细化的分配和调度资源。这里主要以当前最新的2.0展开。感兴趣的同学可以自行了解一下两者差异。
 HDFS：分布式文件系统
 Mapreduce：分布式运算编程框架
 YARN：运算资源调度系统
  简单描述一下三者关系：自定义的MapReduce程序（Java进程），通过YARN框架分配资源来运行这个MapReduce程序，MapReduce程序中输出存储到HDFS。
YARN框架 Apache Hadoop YARN （Yet Another Resource Negotiator，另一种资源协调者）是一种新的 Hadoop 资源管理器，它是一个通用资源管理系统，可为上层应用提供统一的资源管理和调度。它将资源管理和处理组件分开，它的引入为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处。
YARN框架几个重要组件/概念  ResourceManager(RM)  ResourceManager包含两个主要的组件：定时调用器(Scheduler)以及应用管理器(ApplicationsManager (AsM))。
 定时调度器(Scheduler)：这个组件完全是插拔式的，用户可以根据自己的需求实现不同的调度器，目前YARN提供了FIFO、容量以及公平调度器。这个组件的唯一功能就是给提交到集群的应用程序分配资源，并且对可用的资源和运行的队列进行限制。Scheduler并不对作业进行监控，并且它不保证会重启由于应用程序本身或硬件出错而执行失败的应用程序。
 为所有AM分配资源
 应用管理器(ApplicationsManager (AsM))：这个组件用于管理整个集群应用程序的application masters，负责接收应用程序的提交；为application master启动提供资源；监控应用程序的运行进度以及在应用程序出现故障时重启它。
 用于管理所有AM
   NodeManager(NM)  是YARN中每个节点上的代理，它管理Hadoop集群中单个计算节点，根据相关的设置来启动容器的。NodeManager会定期向ResourceManager发送心跳信息来更新其健康状态。同时其也会监督Container的生命周期管理，监控每个Container的资源使用（内存、CPU等）情况，追踪节点健康状况，管理日志和不同应用程序用到的附属服务（auxiliary service）。
 ApplicationMaster(AM)  ApplicationMaster是应用程序级别的，每个ApplicationMaster管理运行在YARN上的应用程序。YARN 将 ApplicationMaster看做是第三方组件，ApplicationMaster负责和ResourceManager scheduler协商资源，并且和NodeManager通信来运行相应的task。ResourceManager 为 ApplicationMaster 分配容器，这些容器将会用来运行task。ApplicationMaster 也会追踪应用程序的状态，监控容器的运行进度。当容器运行完成， ApplicationMaster 将会向 ResourceManager 注销这个容器；如果是整个作业运行完成，其也会向 ResourceManager 注销自己，这样这些资源就可以分配给其他的应用程序使用了。
 Container容器  Container是一个逻辑上的概念。与特定节点绑定的，其包含了内存、CPU磁盘等逻辑资源。不过在现在的容器实现中，这些资源只包括了内存和CPU。容器是由 ResourceManager scheduler 服务动态分配的资源构成。容器授予 ApplicationMaster 使用特定主机的特定数量资源的权限。ApplicationMaster 也是在容器中运行的，其在应用程序分配的第一个容器中运行。
YARN框架执行流程 1.Client执行main()函数中runjob()，向yarn提交job，开启作业。
 这里client jvm是个独立运行的Java进程，跑在Hadoop集群的master节点上。
 2.Client向RM发送作业请求同时RM将作业id以及jar包存放路径返回给Client。
3.Client会把Jar路径为前缀作业id为后缀作为唯一存放路径，将jar包以及输入分片写入到HDFS集群中。
 2,3两步都是Hadoop框架来做的，用户只需执行1即可。注意，这里框架只会默认上传MapReduce程序jar包和输入分片文件。如果用户运行MapReduce程序过程中还需其他的jar包、配置文件等，需要在1中自行上传，以及在task运行起来后自行获取。
 4.Client再次将Job对象（包含2，3存放的路径）提交给RM。
5.RM将其放入调度器，向NM发送命令，NM开启MRAPPMaster进程。
6.MR初始化job。
7.获取3中写入到HDFS中的输入分片。
 每个输入分片会创建一个map任务对象
 8.拿到输入分片后向RM的资源调度器请求分配资源来运行map、reduce task。
9.一旦RM的资源调度器为任务分配了container，AM就通过与NM通信来启动container。
10.该任务由主类为YarnChild的Java进程执行。但是它在运行之前会将需要的资源文件本地化。这里就会去HDFS获取程序运行所需的相关文件。
 在3中补充提到与框架无关的文件需要用户自己去上传和下载。文件是存放在HDFS上，依赖Hadoop的分布式缓存机制进行上传和下载。达到的效果是把用户需要使用的文件分发到各个NM节点。
 11.运行map任务或者reduce任务。
MapReduce模型  图片来自 https://blog.csdn.net/aijiudu/article/details/72353510
 MapReduce将复杂的并行计算过程高度的抽象到了两个函数：Map函数和Reduce函数。
   函数 输入 输出 说明     Map &amp;lt;键1,值1&amp;gt;
如&amp;lt;行号,pid&amp;gt; 如 1.将split后的每个input数据集进一步解析成一批对，输入到map函数进行处理
2.每个输入都会输出，这是中间结果，作为map的输出存储到磁盘。等到job完全结束才会删除。   Reduce 如 写hdfs/disk如solrclient.add() 这里的reduce函数无返回对象，直接在函数内部进行最终结果的输出。    MapReduce的核心是“分而治之”策略。数据在其MapReduce的生命周期中过程中需要经过六大阶段，Input、Split、Map、Shuffule、Reduce和Output。这里讲一下最核心的三个。
Map阶段 1.输入文件进行split成小文件，被每个map任务使用。
2.由程序内的InputFormat(默认实现类TextInputFormat)来读取外部数据，它会调用RecordReader(它的成员变量)的read()方法来读取，返回k,v键值对。k,v键值对传送给map()方法，作为其入参来执行用户定义的map逻辑。
 MapReduce框架默认实现
 3.map()方法输出结果也是k,v键值对。对于map输出的每一个键值对，系统都会给定一个partition，partition值默认是通过计算key的hash值后对Reduce task的数量取模获得。如果一个键值对的partition值为1，意味着这个键值对会交给第一个Reducer处理。用户可以自定义partition逻辑。
4.Map的输出结果是由collector处理的，每个Map任务不断地将键值对输出到在内存中构造的一个环形数据结构中。使用环形数据结构是为了更有效地使用内存空间，在内存中放置尽可能多的数据。环形缓冲区其实就是一个数组，后端不断接受数据的同时，前端数据不断被溢出，长度用完后读取的新数据再从前端开始覆盖。这个缓冲区默认大小100M，可以通过MR.SORT.MB(应该是它)配置。
5.Map按照定义的partitioner分区，每个分区的输出value进行sort排序。
 若有combiner会先执行combiner。combiner作用是预合并，减少输出文件数量和大小。sort也是为了后面merge时效率更高。
 6.按照定义的partitioner分区spill(溢出)到文件（对于某一个小文件，其key都是来自一个partitioner分区），这些文件仍在map task所处机器上。
7.小文件执行merge(合并)，形成partitioner分区内有序的大文件。这些大文件仍在map task所处机器上。
 一个map最终会输出一个output文件，里面存储了所以value值的有序集合（会有不同key的value）。这里输出的文件可以选择使用压缩。
 Reduce阶段 1.copy数据。Reduce从AM那边获取本Reduce的数据存放在哪些Map里，到对应的Map机器去拉取文件。数据被reduce提走之后，map机器不会立刻删除数据，这是为了预防reduce任务失败需要重做。因此map输出数据是在整个作业完成之后才被删除掉的。
2.Merge、Sort、spill。这个过程与Map阶段类似，reduce将copy过来的数据文件加载到内存，进行merge和sort，当内存缓冲区的使用达到阈值后开始往磁盘spill。这个过程会循环多次，最终得到一份有序的reduce输入看k,v键值对文件。
3.通过GroupingComparator()分辨同一组的数据，把他们发送给reduce(k,iterator)方法。
4.调用context.write()方法，会让OutPurFormat调用RecodeWriter的write()方法将处理结果写入到数据仓库中。
 如何输出，输出到哪，都可以在RecodeWriter里自定义。
 Shuffle 这里讲一下shuffle。这个阶段是MapReduce框架中间过程数据混洗阶段，交叉在map和reduce的过程中，包括数据分区，排序，局部聚合，缓存，拉取，再合并排序等阶段。从Map阶段的3~7以及Reduce阶段的1~2都是shuffle阶段。
HDFS Hadoop Distributed File System(hadoop分布式文件系统)，架构图如下。
有四部分组成
Client 职责：
 文件切分。文件上传 HDFS 的时候，Client 将文件切分成 一个一个的Block，然后进行存储。
 与 NameNode 交互，获取文件的位置信息。
 与 DataNode 交互，读取或者写入数据。
 Client 提供一些命令来管理 HDFS，比如启动或者关闭HDFS。
 Client 可以通过一些命令来访问 HDFS。
  NameNode 名称节点，是负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。 你可以把它理解成大管家，它不负责存储具体的数据。
 FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据 操作日志文件EditLog中记录了所有针对文件的创建、删除、重命名等操作 注意，这个两个都是文件，也会加载解析到内存中。  职责：
 管理 HDFS 的名称空间
 管理数据块（Block）映射信息
 配置副本策略
 处理客户端读写请求。
  SendaryNamenode NameNode节点的备份，它会定期的和namenode就行通信来完成整个的备份操作。并非 NameNode 的热备。当NameNode 挂掉的时候，它并不能马上替换 NameNode 并提供服务。
职责：
 辅助 NameNode，分担其工作量。
 定期合并 fsimage和fsedits，并推送给NameNode。
 在紧急情况下，可辅助恢复 NameNode。
  DataNode 数据节点，用来具体的存储文件，维护了blockId 与 datanode本地文件的映射。 需要不断的与namenode节点通信，来告知其自己的信息，方便nameode来管控整个系统。
数据节点存储数据以块为单位，块大小默认是128m，默认会存储多份。
职责：
 存储实际的数据块。 执行数据块的读/写操作。  Hadoop集群节点组成 Hadoop集群逻辑节点组成 包括HDFS集群和YARN框架集群：
 HDFS集群  NameNode
SecondNameNode
DataNode
 YARN框架集群  ResourceManageNode
NodeManageNode
Hadoop集群物理节点应该如何分配？ 例如一套5节点的Hadoop集群：
节点1部署NameNode和ResourceManageNode（一般NameNode和ResourceManageNode部署在同一个节点上）这个节点为主节点。client node也在这个节点上；
节点2，3，4，5部署DataNode和NodeManageNode；
节点2部署SecondNameNode。
EMR集群介绍 EMR是AWS提供的Hadoop集群服务。现在使用云服务是主流趋势，这里介绍一下EMR集群节点组成。
 Master node  该节点管理集群，它通过运行软件组件来协调在其他节点之间分配数据和任务的过程以便进行处理。主节点跟踪任务的状态并监控集群的健康状况。每个集群具有一个主节点，并且可以创建仅包含主节点的单节点集群。
 Core node  该节点具有运行任务并在集群上的 Hadoop 分布式文件系统 (HDFS) 中存储数据的软件组件。多节点集群至少具有一个核心节点。
 Task node  该节点具有仅运行任务但不在 HDFS 中存储数据的软件组件。任务节点是可选的。
问题  Hadoop和MapReduce是什么关系？  我在搜索Hadoop和MapReduce的相关资料时会遇到到底搜“Hadoop”关键字还是“MapReduce”关键字。现在网上的博客很多也没有区分两者关系。
狭义Hadoop：本文中我们提到的Hadoop都算狭义Hadoop，MapReduce是一种运算框架，Hadoop包含MapReduce。
广义Hadoop：指Hadoop生态圈，其包括Hadoop、Hadoop Yarn、Hadoop MapReduce、Hadoop HDFS、Hive、Hbase、ZooKeeper、Spark等。
 参考文章
https://blog.csdn.net/aijiudu/article/details/72353510
https://blog.csdn.net/qianbing11/article/details/82357033
https://blog.csdn.net/xiaoshunzi111/article/details/48810239
https://blog.csdn.net/weixin_38750084/article/details/82963235
</content>
    </entry>
    
     <entry>
        <title>solr suggest实战</title>
        <url>https://jasonqian10.github.io/post/solr-suggest/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>lucene</tag><tag>suggest</tag>
        </tags>
        <content type="html"> solr suggest模块介绍 自动提示功能在现今的互联网产品中的应用几乎遍地都是。比如在京东主页搜索”苹果“，输入框下面就会自动联想出与苹果相关的关键词。
本文我们主要就来讲一下solr是如何去实现这样的自动提示功能。
Solr 中是通过 SuggestComponent 模块 为用户提供查询关键字的自动提示功能。可以使用此功能在搜索应用程序中实现强大的自动提示功能。
如何使用suggest模块 官方使用方式 先来介绍一下官方给出的使用方式，在solrconfig.xml中添加如下配置
&amp;lt;requestHandler name=&amp;#34;/suggest&amp;#34; class=&amp;#34;solr.SearchHandler&amp;#34; startup=&amp;#34;lazy&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;defaults&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;suggest&amp;#34;&amp;gt;true&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggest.count&amp;#34;&amp;gt;10&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;arr name=&amp;#34;components&amp;#34;&amp;gt; &amp;lt;str&amp;gt;suggest&amp;lt;/str&amp;gt; &amp;lt;!--这里配置的就是下面的searchComponent “suggest”--&amp;gt; &amp;lt;/arr&amp;gt; &amp;lt;/requestHandler&amp;gt;&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;FuzzyLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;DocumentDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;cat&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;weightField&amp;#34;&amp;gt;price&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;string&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt; 配置是什么作用 与是组合使用的，requestHandler定义的name &amp;ldquo;suggest&amp;rdquo;表示请求handler，当请求URL中出现&amp;rdquo;suggest&amp;rdquo;时，例如http://localhost:8983/solr/techproducts/suggest ，就会使用如上配置，进入solr.SearchHandler类，然后进入searchComponent配置的solr.SuggestComponent类。
searchComponent配置中几个重要的参数含义
 name：suggester名。可以任意取。当searchComponent中配置多个suggester时，可以通过查询参数 suggest.dictionary=mySuggester 指定查询哪一个suggester。且在response中会根据suggester名显示结果。 lookupImpl：查找实现。query词典和build词典的逻辑由其完成。solr提供了多种实现类。 dictionaryImpl：字典实现。词典中各字段值是由其获取的。solr提供了多种实现类。 field：指定主索引中的字段，需要stored，这个字段值用于生成suggest词典。 weightField：排序字段，需要数字类型，suggest结果会根据这个字段数值进行排序展示。 suggestAnalyzerFieldType：分析字段。lookupImpl类query和build阶段进行分析。 buildOnStartup：是否solr启动时构建suggest词典。建议false，否则启动很耗性能，可以通过suggest.build=true手动触发。 buildOnCommit和buildOnOptimize：是否在soft-commit和optimize后重建词典。可以根据自己soft-commit和optimize的频率来决定是否启用。如果频率比较高，建议false。可以通过suggest.build=true手动触发。  如何定制 上面讲的是官方给出的使用方式，当然使用上面的配置，能实现基本的suggest功能。下面讲讲我的项目中是如何定制suggest的。下面分别介绍两个suggest例子：
关键字自动提示 需求：用户输入关键字，前缀匹配出相关性结果，根据词频大小排序展示给用户。
我们把用户的查询关键字清洗出来，索引关键字和词频，用这份主索引生成词典。查询suggest词典，根据词频大小排序展示给用户。demo效果如下
searchComponent配置
&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34; enable=&amp;#34;true&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;AnalyzingInfixLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;indexPath&amp;#34;&amp;gt;index_keyword&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;highlight&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;DocumentDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;keyword&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;weightField&amp;#34;&amp;gt;tf&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;sortField&amp;#34;&amp;gt;tf&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;text_suggest&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnCommit&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnOptimize&amp;#34;&amp;gt;true&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt;  text_suggest参考官网配置
&amp;lt;fieldType name=&amp;#34;text_suggest&amp;#34; class=&amp;#34;solr.TextField&amp;#34; sortMissingLast=&amp;#34;true&amp;#34; omitTermFreqAndPositions=&amp;#34;true&amp;#34;&amp;gt; &amp;lt;analyzer&amp;gt; &amp;lt;tokenizer class=&amp;#34;solr.KeywordTokenizerFactory&amp;#34;/&amp;gt; &amp;lt;filter class=&amp;#34;solr.LowerCaseFilterFactory&amp;#34; /&amp;gt; &amp;lt;/analyzer&amp;gt; &amp;lt;/fieldType&amp;gt;  这里KeywordTokenizerFactory不会进行分词，会把输入文本作为完整的关键字进行索引。
为何选择AnalyzingInfixLookupFactory和DocumentDictionaryFactory的组合？
 AnalyzingInfixLookupFactory  主要在于AnalyzingLookupFactory与AnalyzingInfixLookupFactory区别。两者都是支持字段分析，后者优势在于支持前缀匹配（prefix matches），例如词典中存在关键字apple，当搜索app就能匹配到关键字apple。我的应用场景需要支持前缀匹配。
AnalyzingInfixLookupFactory支持indexPath配置，其他lookup不支持。
 DocumentDictionaryFactory  这个字典实现支持weightField配置，我们的应用场景需要根据关键字的词频进行推荐排序，正好可以通过weightField实现。
公司名自动提示 需求：本例子与上面的关键字提示有点不同，上面的例子是前缀匹配，本例需要短语匹配，只要命中公司中任意部分都会匹配上。根据文档频率排序展示。demo类似于天眼查搜索
searchComponent配置
&amp;lt;searchComponent name=&amp;#34;suggest&amp;#34; class=&amp;#34;solr.SuggestComponent&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;suggester&amp;#34;&amp;gt; &amp;lt;str name=&amp;#34;name&amp;#34;&amp;gt;mySuggester&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;lookupImpl&amp;#34;&amp;gt;com.test.AnalyzingInfixPhraseQueryLookupFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;dictionaryImpl&amp;#34;&amp;gt;HighFrequencyDictionaryFactory&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;indexPath&amp;#34;&amp;gt;index_company&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;field&amp;#34;&amp;gt;COMPANY&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;suggestAnalyzerFieldType&amp;#34;&amp;gt;suggest_text&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;highlight&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;int name=&amp;#34;minPrefixChars&amp;#34;&amp;gt;100&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;buildOnStartup&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;buildOnCommit&amp;#34;&amp;gt;false&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/searchComponent&amp;gt;  AnalyzingInfixPhraseQueryLookupFactory：这是个定制类，目的是为了实现短语查询
 suggest_text为
&amp;lt;fieldType name=&amp;#34;suggest_text&amp;#34; class=&amp;#34;solr.TextField&amp;#34; positionIncrementGap=&amp;#34;100&amp;#34;&amp;gt; &amp;lt;analyzer&amp;gt; &amp;lt;tokenizer class=&amp;#34;solr.StandardTokenizerFactory&amp;#34; /&amp;gt; &amp;lt;filter class=&amp;#34;solr.ICUTransformFilterFactory&amp;#34; id=&amp;#34;Traditional-Simplified&amp;#34;/&amp;gt; &amp;lt;filter class=&amp;#34;solr.LowerCaseFilterFactory&amp;#34; /&amp;gt; &amp;lt;/analyzer&amp;gt; &amp;lt;/fieldType&amp;gt;  这里会进行分词处理。查询时通过PhraseQuery进行搜索。
 HighFrequencyDictionaryFactory：这个词典类作用是能获取term的docFreq，把docFreq作为weight字段的值，我们的需求需要根据公司名的docFreq来排序展示。  suggest源码分析 suggest query源码执行时序图 suggest build源码执行时序图 suggest索引是在主索引基础上build出来的（通过HTTP请求参数suggest.build=true），build时序图即是suggest构建索引的过程。
suggest模块几个重要的类  SolrSuggester  solr初始化的时候，SolrSuggester类会维护lookupImpl和dictionaryImpl对象，query和build请求都会先经过SolrSuggester类。主要两个函数
a).getSuggestions()：query请求会走这个函数，调用lookupImpl对象的lookup()
b).build()：build请求走这个函数，调用lookupImpl对象的build()
 lookupImpl  lookupImpl类都是继承Lookup类，实现lookup()和build()。其中build()会调用dictionaryImpl对象的next()
 dictionaryImpl  dictionaryImpl类实现Dictionary接口，对象内部维护迭代器，迭代器实现InputIterator接口的next()，遍历获取所需字段。
</content>
    </entry>
    
     <entry>
        <title>solr搜索原理解析</title>
        <url>https://jasonqian10.github.io/post/solr-lucene-principle/</url>
        <categories>
          <category>Search</category>
        </categories>
        <tags>
          <tag>solr</tag><tag>lucene</tag>
        </tags>
        <content type="html"> solr与Lucene的关系 讲搜索流程之前先介绍一下solr与Lucene的关系。
Lucene是一个索引与搜索类库，而不是完整的程序。使用Lucene的方式主要有二种：一是自己编写程序，调用类库；二是使用第三方基于Lucene编写的程序，如下面介绍的Solr等。
Solr 是一个开源的搜索服务器，Solr 使用 Java 语言开发，主要基于 HTTP 和 Apache Lucene 实现。Solr是在Lucene上封装的完善的搜索引擎。
solr是门户，lucene是底层基础。通俗地说，如果Solr是汽车，那么Lucene就是发动机，没有发动机，汽车就没法运转，但对于用户来说只可开车，不能开发动机。
solr搜索流程分solr部分和Lucene部分，整体流程是请求先经过solr部分再进入Lucene部分。  说明 对应网上已有的素材或者文字符合作者想要的描述，就直接引用了，不重复造轮子
参考 https://www.cnblogs.com/forfuture1978/archive/2010/04/04/1704282.html
https://www.cnblogs.com/davidwang456/p/10570935.html https://blog.csdn.net/huangzhilin2015/article/details/89372127
 solr的启动过程 Solr可以独立运行，运行在Jetty中，Jetty 是一个开源的servlet容器，它为基于Java的web容器，其工作流程（也就是solr server启动过程）如下 这属于servlet范畴，本文不重点讨论，大家知道大体流程即可。
solr query流程 solr处理query的入口是SolrDispatchFilter，其实现了javax.servlet的Filter的接口。通过拦截servlet请求的方式进入solr处理。 上图可以看出solr中的流程。
下面按照处理顺序重点讲一下solr几个处理query的核心类：
   核心类 如何生效 solrconfig.xml声明方式(例子)     SearchHandler 通过查询参数”qt“指定requestHandler的name，如本例中/test   test_type    SearchComponent 在SearchHandler类初始化时注册的name。如本例中TestSearchHandler类中注册”test_query“    QueryComponet 处理查询的类，属于SearchComponent的其中一类。    QParser 在QueryComponet进行查询处理时会先创建query对象分析器QParser。生效方式：1）配置在requestHandler下元素defType；2）通过查询参数defType指定     SearchHandler 真正处理请求的入口函数在SearchHandler.handleRequestBody()。对于不同的Request-Handler(qt)，请求会进入不同的SearchHandler，这由solrconfig.xml中的配置决定。我们这里以 select 请求为例，请求会进入SearchHandler。handleRequestBody()中主要做的事情就是依次调用SearchComponent列表的prepare，process，post方法。  如何定制SearchHandler   当然可以定制自己的Request-Handler，继承SearchHandler或RequestHandlerBase都可以。然后在solrconfig.xml中标签中配置自己开发的类。例如自定义TestSearchHandler，url path为/test，则样例配置如下
&amp;lt;requestHandler name=&amp;#34;/test&amp;#34; class=&amp;#34;com.test.TestSearchHandler&amp;#34;&amp;gt; &amp;lt;lst name=&amp;#34;defaults&amp;#34;&amp;gt; &amp;lt;int name=&amp;#34;timeAllowed&amp;#34;&amp;gt;11000&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;echoParams&amp;#34;&amp;gt;none&amp;lt;/str&amp;gt; &amp;lt;int name=&amp;#34;rows&amp;#34;&amp;gt;20&amp;lt;/int&amp;gt; &amp;lt;str name=&amp;#34;defType&amp;#34;&amp;gt;xxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;op&amp;#34;&amp;gt;AND&amp;lt;/str&amp;gt; &amp;lt;float name=&amp;#34;tie&amp;#34;&amp;gt;1&amp;lt;/float&amp;gt; &amp;lt;str name=&amp;#34;qf&amp;#34;&amp;gt;xxxxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;fl&amp;#34;&amp;gt;xxxxx&amp;lt;/str&amp;gt; &amp;lt;str name=&amp;#34;sort&amp;#34;&amp;gt;score desc,_version_ desc&amp;lt;/str&amp;gt; &amp;lt;/lst&amp;gt; &amp;lt;/requestHandler&amp;gt; SearchComponent 初始化的时候SearchHandler中注册了本SearchHandler对应的SearchComponent的列表。一般包含主要功能的QueryComponent、FacetComponent、HighlightComponent、DebugComponent等，这些Component类都是继承SearchComponent这个抽象类。
SearchComponent抽象类定义了三个阶段prepare，process，post，SearchHandler.handleRequestBody()中会遍历所有注册的SearchComponent，调用这三个阶段，完成各个SearchComponent中功能。
 如何定制SearchComponent   定制SearchComponent的方法与定制SearchHandler的方法类似。继承SearchComponent或者对应功能的SearchComponent（如QueryComponent、FacetComponent、HighlightComponent、DebugComponent等）。然后在solrconfig.xml中标签配置自己开发的类。样例配置如下
&amp;lt;searchComponent name=&amp;#34;test_componet&amp;#34; class=&amp;#34;com.test.solr.TestComponent&amp;#34;&amp;gt; xxxxxx 里面可以配置自定义的参数 &amp;lt;/searchComponent&amp;gt; QueryComponet 进入查询最核心的QueryComponent，solr查询请求功能都是在本类中完成。 &#43; QueryComponent.prepare()
根据参数defType(配置在solrconfig.xml中标签中，不配置的话默认是”lucene“)，初始化QParser以及Query。以defType=”lucene“为例，其初始化过程如下
1.QParser parser = QParser.getParser(rb.getQueryString(), defType, req) 2.LuceneQParserPlugin.createParser(qstr, localParams, req.getParams(), req) 3.new LuceneQParser(qstr, localParams, params, req); 4.Query query = LuceneQParser.parse()  补充：QParser类与QParserPlugin类 QParser类的作用是QParser.parse() 可以构造出Query对象，查询时必须使用的对象。QParser一般由QParserPlugin.createParser()创建。QParser类与QParserPlugin类都是抽象类，不同的defType参数，对应不同的实现类。以defType=”lucene“为例，其初始化过程如下
  QueryComponent.process()   process()主要调用SolrIndexSearcher.search()，主要工作都在search()中完成。
 SolrIndexSearcher类继承IndexSearcher，对InderSearch做了封装。最终调用IndexSearcher.search()函数。 这是Lucene处理query的入口。
 Lucene query流程 Lucene处理query的入口是IndexSearcher.search()。其调用流程如下 createNormalizedWeight 创建归一化weight的流程，包括
重写Query对象 代码为：query = rewrite(query)
这个rewrite()作用是将 2.3 节中QParse.parse()解析出的Query对象转换成Query对象树，这棵树很重要，从Query对象树——》Weight对象树——》Scorer对象树，一直贯穿整个索引过程。为什么需要rewrite？ 因为solr很多不同的查询类型，比如前缀查询和通配符查询，从本质上，任何的查询都可以视为对多个关键词的查询。整个重写过程是把从Lucene角度认为原始的、开销大的查询对象转变成一系列开销小的查询对象的一个过程。
举个例子，查询语句 Title:(car* or bike) ，其QParse.parse()解析出的Query对象结构为
BooleanQuery(Title:(car* or bike)) &#43; BooleanQuery(Title:(car* or bike)) &#43; PrefixQuery(Title:car*) &#43; TermQuery(Title:or) --这个BooleanClause后面流程会继续处理掉 &#43; TermQuery(Title:bike) rewrite Query之后为
BooleanQuery(Title:(car* or bike)) &#43; MultiTermQueryConstantScoreWrapper(Title:car*) &#43; PrefixQuery(Title:car*) &#43; TermQuery(Title:or) --这个BooleanClause后面流程会继续处理掉 &#43; TermQuery(Title:bike) 对于PrefixQuery和FuzzyQuery，这些查询语句由于特殊的语法，可能对应的不是一个词，而是多个词，因而他们都有rewriteMethod对象指向MultiTermQuery的Inner Class，表示对应多个词，在查询过程中会得到特殊处理。
创建weight 调用 Query.createWeight(Searcher) 创建weight，以3.1.1中BooleanQuery为例，代码为
BooleanQuery.createWeight(Searcher) ... return new BooleanWeight(searcher) BooleanWeight构造函数主要实现是递归遍历Query树，生成Weight树。遍历过程中叶子节点是TermQuery，其TermQuery.createWeight(Searcher) 返回return new TermWeight(searcher)对象。在TermWeight构造函数中，需要做几件事：
 获取Similarity类
this.similarity = searcher.getSimilarity(needsScores); Similarity类是Lucene的相似度类（用于计算文档分数），嵌套在Weight对象中，Weight对象嵌套在Query对象中。这样Query树中每个query节点都会构造自己的打分类。 &amp;gt; 补充： &amp;gt; searcher.getSimilarity() 获取的Similarity对象是在初始化solr时创建的，根据schema.xml中是否定义了Similarity类，如果定义了，则用用户自定义的Similarity类进行打分，如果没有自定义，则使用solr默认的打分类BM25Similarity。后面的文章会详细介绍如何自定义打分。
 计算idf
this.stats = similarity.computeWeight(collectionStats, termStats); 这里 this.stats 对象已经计算好idf值。
   Lucene打分使用TF-IDF的打分公式，idf是其中一项。这里不详细介绍。
 计算分数 代码为
float v = weight.getValueForNormalization(); float norm = getSimilarity(needsScores).queryNorm(v); weight.normalize(norm, 1.0f); 这步计算的TF-IDF打分公式中仅与搜索语句相关与文档无关的部分(即不依赖于查询结果)，每个query子对象分数都是一样。分数存在weight对象中，在最终打分时可以直接使用。因为与文档无关，无需收集文档时遍历每篇文档重复计算，这里计算好，后面重复使用。
收集(collect)文档和打分(score) 收集文档 文档收集是收集匹配query的文档集，这里重点介绍涉及的几个类。  文档收集器（Collector及其实现类） 如上图Collector类是文档收集器接口类，其getLeafCollector()方法来获取LeafCollector对象（段文档收集器）。 TopDocsCollector类是个抽象类，实现了Collector接口，是一个文档收集器基类。其有个TopScoreDocCollector#create方法用于创建文档收集器对象，这个创建出来的对象一般是TopScoreDocCollector的子类。 SimpleTopScoreDocCollector类，是TopScoreDocCollector的子类，也是其内部类，是简单查询文档收集器。 PagingTopScoreDocCollector类，是TopScoreDocCollector的子类，也是其内部类，是用于分页查询文档收集器。
 段文档收集器（LeafCollector类及其实现类） LeafCollector是段文档收集器接口，LeafCollector#collect 方法最终完成收集文档的工作。段文档收集器会被Collector#getLeafCollector 初始化，包装在文档收集器中。这里会用到其子类ScorerLeafCollector。
  介绍好了这几个类之后，开始讲收集文档的过程。首先还在2.3步的时候，SolrIndexSearcher.search()方法里会调用SolrIndexSearcher#getDocListNC方法，在该方法中调用
final TopDocsCollector topCollector = buildTopDocsCollector(len, cmd); SolrIndexSearcher#buildTopDocsCollector方法中调用
TopScoreDocCollector.create(weightedSort, len, searchAfter, fillFields, needScores, needScores); 注意：当query参数中存在sort字段是会用。这里以上面的为例。 TopFieldCollector.create(weightedSort, len, searchAfter, fillFields, needScores, needScores); 这个方法在上面介绍TopScoreDocCollector类的时候提过到。TopScoreDocCollector#create中会创建TopScoreDocCollector的子类。当简单查询时创建SimpleTopScoreDocCollector，分页查询时创建PagingTopScoreDocCollector。
1. new SimpleTopScoreDocCollector(numHits); 2. new PagingTopScoreDocCollector(numHits, after); 然后会接下去会调用IndexSearcher#search(List leaves, Weight weight, Collector collector)方法，该方法的处理逻辑是for循环每个段，执行
leafCollector = collector.getLeafCollector(ctx); 获取段文档收集器对象leafCollector，这里的collector就是上面创建的SimpleTopScoreDocCollector类对象。
创建Scorer树 Scorer类是Lucene实现打分的类，是个抽象类，每个Query子类都会集成Scorer类，作为每个Query打分的入口。如TermQuery的Scorer类是TermScorer，TermQuery打分的入口则是TermScorer#score()。
创建Scorer树入口代码：
BulkScorer scorer = weight.bulkScorer(ctx) 创建Scorer对象树的过程其实与创建Weight对象树的过程类似。遍历Weight树依次创建Scorer对象。以TermQuery对象为例，初始化关键代码：
1.Scorer scorer = TermWeight.scorer() 2.return new TermScorer(this, docs, similarity.simScorer(stats, context)); 创建TermScorer时传入了SimScorer对象，由Similarity#simScorer获取而来，Similarity类的对象，在创建TermQuery时维护在对象内部。
similarity.simScorer(stats, context) SimScorer类是Similarity类的内部类，具体打分逻辑就是SimScorer#score()方法中实现。TermScorer对象初始化时会在对象内部维护SimScorer对象。打分时调用顺序会是： TermScorer.score() ——》 SimScorer.score()
打分score 打分入口是BulkScorer#score()方法
scorer.score(leafCollector, ctx.reader().getLiveDocs()); 进入Weight#score()方法，这里会执行
collector.setScorer(scorer); 为收集器对象传入scorer打分对象（这个scorer对象会最终被LeafCollector#collect中使用来进行打分）。 调用Weight#scoreAll()方法，这里会调用DocIdSetIterator#nextDoc()方法，DocIdSetIterator是个迭代器，依次拿出匹配到的文档id。
int doc = iterator.nextDoc()  至于如何拿出这些匹配到文档id，过程比较复杂，涉及Lucene索引文件相关的原理，后面文章会细聊。
 这里需要注意，每个段拿出的文档id是基于本段的排序。而最终的文档id是需要全局唯一的，这里需要加上docBase。
final int docBase = context.docBase; pqTop.doc = doc &#43; docBase; 然后传入拿出的文档id，调用段文档收集器LeafCollector#collect 方法
collector.collect(doc); 然后进入 TermScorer.score() ——》 SimScorer.score()完成打分。
</content>
    </entry>
    
</search>