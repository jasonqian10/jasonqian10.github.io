<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 知不道</title>
    <link>https://jasonqian10.github.io/post/</link>
    <description>Recent content in Posts on 知不道</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Sun, 22 Dec 2019 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://jasonqian10.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>【Hadoop索引】hadoop调优经验</title>
      <link>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-3/</link>
      <pubDate>Sun, 22 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-3/</guid>
      <description>&lt;p&gt;【Hadoop索引】本篇系列第三篇。主要介绍Hadoop调试过程中的一些问题以及解决方法的总结。&lt;/p&gt;

&lt;p&gt;主要有以下几类问题&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;资源分配问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;性能调优问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Yarn框架运行机制问题&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Solr索引问题&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>【Hadoop索引】Mapreduce进行solr索引实战</title>
      <link>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-2/</link>
      <pubDate>Fri, 06 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-2/</guid>
      <description>&lt;p&gt;【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第二篇，介绍如何应用Hadoop到solr index。&lt;/p&gt;

&lt;h2 id=&#34;为什么使用hadoop来做solr索引&#34;&gt;为什么使用Hadoop来做solr索引？&lt;/h2&gt;

&lt;p&gt;这与Hadoop特点有关，hadoop适用于处理大数据并行计算，提高solr索引速度。Hadoop的MapReduce计算框架分map阶段和reduce阶段。map阶段转换solr document，reduce阶段进行solr index操作。MapReduce流程契合solr索引的流程。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>【Hadoop索引】我理解的hadoop</title>
      <link>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-1/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/mapreduce/mapreduce-index-1/</guid>
      <description>&lt;p&gt;【Hadoop索引】将介绍如何使用Hadoop框架进行solr索引。本篇系列第一篇，介绍我学习使用Hadoop后对其的理解。&lt;/p&gt;

&lt;h2 id=&#34;hadoop概念&#34;&gt;Hadoop概念&lt;/h2&gt;

&lt;h3 id=&#34;官方概念&#34;&gt;官方概念&lt;/h3&gt;

&lt;p&gt;The Apache Hadoop software library is a framework that allows for the distributed processing of large data sets across clusters of computers using &lt;strong&gt;simple programming&lt;/strong&gt; &lt;strong&gt;models&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;总结：处理大数据、分布式集群、可编程计算框架&lt;/p&gt;

&lt;h3 id=&#34;hadoop-的核心组件&#34;&gt;Hadoop 的核心组件&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;https://jasonqian10.github.io/img/post/image-20191206112523725.png&#34; alt=&#34;image-20191206112523725&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hadoop由1.0和2.0两个架构版本，2.0最大的变化就是引入YARN这个资源调度框架，更加精细化的分配和调度资源。这里主要以当前最新的2.0展开。感兴趣的同学可以自行了解一下两者差异。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;HDFS：分布式文件系统&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Mapreduce：分布式运算编程框架&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;YARN：运算资源调度系统&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单描述一下三者关系：自定义的MapReduce程序（Java进程），通过YARN框架分配资源来运行这个MapReduce程序，MapReduce程序中输出存储到HDFS。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>solr suggest实战</title>
      <link>https://jasonqian10.github.io/post/solr-suggest/</link>
      <pubDate>Fri, 15 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/solr-suggest/</guid>
      <description>&lt;h3 id=&#34;solr-suggest模块介绍&#34;&gt;solr suggest模块介绍&lt;/h3&gt;

&lt;h4 id=&#34;介绍&#34;&gt;介绍&lt;/h4&gt;

&lt;p&gt;自动提示功能在现今的互联网产品中的应用几乎遍地都是。比如在京东主页搜索”苹果“，输入框下面就会自动联想出与苹果相关的关键词。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://jasonqian10.github.io/img/post/image-20191116164420559.png&#34; alt=&#34;image-20191116164420559&#34; /&gt;&lt;/p&gt;

&lt;p&gt;本文我们主要就来讲一下solr里面是如何去实现这样的自动提示功能。&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>solr搜索原理解析</title>
      <link>https://jasonqian10.github.io/post/solr-lucene-principle/</link>
      <pubDate>Fri, 18 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/solr-lucene-principle/</guid>
      <description>&lt;h3 id=&#34;solr与lucene的关系&#34;&gt;solr与Lucene的关系&lt;/h3&gt;

&lt;p&gt;讲搜索流程之前先介绍一下solr与Lucene的关系。&lt;/p&gt;

&lt;p&gt;Lucene是一个索引与搜索类库，而不是完整的程序。使用Lucene的方式主要有二种：一是自己编写程序，调用类库；二是使用第三方基于Lucene编写的程序，如下面介绍的Solr等。&lt;/p&gt;

&lt;p&gt;Solr 是一个开源的搜索服务器，Solr 使用 Java 语言开发，主要基于 HTTP 和 Apache Lucene 实现。Solr是在Lucene上封装的完善的搜索引擎。&lt;/p&gt;

&lt;p&gt;solr是门户，lucene是底层基础。通俗地说，如果Solr是汽车，那么Lucene就是发动机，没有发动机，汽车就没法运转，但对于用户来说只可开车，不能开发动机。&lt;/p&gt;

&lt;p&gt;solr搜索流程分solr部分和Lucene部分，整体流程是请求先经过solr部分再进入Lucene部分。
&lt;img src=&#34;https://jasonqian10.github.io/img/post/solr_lucene-1542234.png&#34; alt=&#34;solr_lucene&#34; style=&#34;zoom:40%;&#34; /&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title></title>
      <link>https://jasonqian10.github.io/post/nlp-tf-idf-bm25/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://jasonqian10.github.io/post/nlp-tf-idf-bm25/</guid>
      <description>title: &amp;ldquo;搜索引擎中相似度算法TF-IDF和BM25&amp;rdquo; date: &amp;ldquo;2019-12-26&amp;rdquo; categories:
 &amp;ldquo;Lucene&amp;rdquo; tags: &amp;ldquo;TF-IDF&amp;rdquo; &amp;ldquo;BM25&amp;rdquo; toc: true  前言 当我们使用搜索引擎时，它总是会把相关性高的内容显示在前面，相关性低的内容显示在后面。那么，搜索引擎是如何计算关键字和内容的相关性呢？这里介绍2种重要的相似度算法：TF-IDF和BM25。
TF-IDF是Lucene上一代（6.0以前）相似度算法，BM25是Lucene新一代（6.0以后）正使用的相似度算法。
先举个例子。假如，我们想找和“Lucene”相关的文章。可以想一下，那些内容里只出现过一次“Lucene”的文章，有可能是在讲某种技术，顺便提到了Lucene这个工具。而那些出现了两三次“Lucene”的文章，很可能是专门讨论Lucene的。通过直觉，我们可以得出判断：关键字出现的次数越多，文档与关键字的匹配度越高。
TF Term Frequency，缩写为TF。通常叫做“词频”，表示文档中关键字出现的次数。
 通常TF越大，相关性越高。
 但是，你可能会发现一个问题。例如一篇小短文里出现了一次“Lucene”，而一部好几百页的书里提到两次“Lucene”，此时我们就不能说后者相关度更高了。为了消除文档本身大小的影响，在计算TF时引入文档长度这个参数，做文档长度标准化
TF socre ＝ 某个词在文档中出现的次数 ／ 文档的长度 举例：某文档D，长度为200，其中“Lucene”出现了2次，“的”出现了20次，“原理”出现了3次，那么
TF(Lucene|D) = 2/200 = 0.01 TF(的|D) = 20/200 = 0.1 TF(原理|D) = 3/200 = 0.015 其中我们发现“的”这个词的比重非常高，搜索引擎中一般把这种“的、地、得”这些虚词去掉，不计算其分数。
IDF Inverse Dcument Frequency, 缩写为IDF。通常叫做“逆文档频率”，其定义为
IDF = log(语料库的文档总数 / 包含该词的文档数 + 1) 可见包含该词的文档数越小，分母就越小，IDF越大；该词越常见，分母就越大，IDF越小越接近0。分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。
举例：世界上文档总数位100亿，&amp;rdquo;Lucene&amp;rdquo;在1万个文档中出现过，“原理”在2亿个文档中出现过，那么它们的IDF值分别为：
IDF(Lucene) = log(100亿/1万+1) = 19.93 IDF(原理) ＝ log(100亿/2亿+1) ＝ 5.</description>
    </item>
    
  </channel>
</rss>